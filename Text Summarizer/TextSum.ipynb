{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextSum",
      "provenance": [],
      "collapsed_sections": [
        "2U8ebeL4rbbO",
        "tkLUNS6rwTZ4",
        "mV2_jNFnT-kr"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UF26J0ZNGi-T",
        "outputId": "22928342-cf6e-4d27-c4ac-441736f72684"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.layers.core import Dense\n",
        "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
        "import time\n",
        "import numpy as np\n",
        "import re\n",
        "!pip install Sastrawi\n",
        "# !pip install tqdm\n",
        "from tqdm import tqdm \n",
        "import Sastrawi\n",
        "import nltk \n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "\n",
        "factory = StopWordRemoverFactory()\n",
        "stopwords = factory.get_stop_words()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Collecting Sastrawi\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/4b/bab676953da3103003730b8fcdfadbdd20f333d4add10af949dd5c51e6ed/Sastrawi-1.0.1-py2.py3-none-any.whl (209kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 8.2MB/s \n",
            "\u001b[?25hInstalling collected packages: Sastrawi\n",
            "Successfully installed Sastrawi-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyggTdNkPFe3",
        "outputId": "f6f98654-77bb-49cb-affb-a021153614dc"
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xs5UMQaHMmEI"
      },
      "source": [
        "### Load & Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_e_XDnFbMZ7H"
      },
      "source": [
        "#mount gdrive\n",
        "path = '/content/drive/MyDrive/indosum.csv'\n",
        "\n",
        "data = pd.read_csv(path)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "unrYc1BwM8EZ",
        "outputId": "a7854ee7-9694-4432-f80c-1f1edf8b1337"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>paragraphs</th>\n",
              "      <th>summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Jakarta , CNN Indonesia - - Dokter Ryan Thamri...</td>\n",
              "      <td>Dokter Lula Kamal yang merupakan selebriti sek...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Selfie ialah salah satu tema terpanas di kalan...</td>\n",
              "      <td>Asus memperkenalkan   ZenFone generasi keempat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Jakarta , CNN Indonesia - - Dinas Pariwisata P...</td>\n",
              "      <td>Dinas Pariwisata Provinsi Bengkulu kembali men...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Merdeka.com - Indonesia Corruption Watch ( ICW...</td>\n",
              "      <td>Indonesia Corruption Watch ( ICW ) meminta Kom...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Merdeka.com - Presiden Joko Widodo ( Jokowi ) ...</td>\n",
              "      <td>Jokowi memimpin upacara penurunan bendera .\\nU...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          paragraphs                                            summary\n",
              "0  Jakarta , CNN Indonesia - - Dokter Ryan Thamri...  Dokter Lula Kamal yang merupakan selebriti sek...\n",
              "1  Selfie ialah salah satu tema terpanas di kalan...  Asus memperkenalkan   ZenFone generasi keempat...\n",
              "2  Jakarta , CNN Indonesia - - Dinas Pariwisata P...  Dinas Pariwisata Provinsi Bengkulu kembali men...\n",
              "3  Merdeka.com - Indonesia Corruption Watch ( ICW...  Indonesia Corruption Watch ( ICW ) meminta Kom...\n",
              "4  Merdeka.com - Presiden Joko Widodo ( Jokowi ) ...  Jokowi memimpin upacara penurunan bendera .\\nU..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIZxtKIwNE6c",
        "outputId": "4e0f5bc1-ab3d-434f-d3ab-5ef6a26b1e5e"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14262, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Paj_Li-8NMs_",
        "outputId": "d891ebb2-dec9-42d3-864f-43be99d30d32"
      },
      "source": [
        "#display some of the data\n",
        "with pd.option_context('display.max_colwidth', None):\n",
        "  for i in range(3):\n",
        "    print(\"Paragraph : \" + data['paragraphs'].values[i])\n",
        "    print(\"Summary : \" + data['summary'].values[i] + \"\\n\")\n",
        "    "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Paragraph : Jakarta , CNN Indonesia - - Dokter Ryan Thamrin , yang terkenal lewat acara Dokter Oz Indonesia , meninggal dunia pada Jumat ( 4 / 8 ) dini hari .\n",
            "Dokter Lula Kamal yang merupakan selebriti sekaligus rekan kerja Ryan menyebut kawannya itu sudah sakit sejak setahun yang lalu .Lula menuturkan , sakit itu membuat Ryan mesti vakum dari semua kegiatannya , termasuk menjadi pembawa acara Dokter Oz Indonesia .\n",
            "Kondisi itu membuat Ryan harus kembali ke kampung halamannya di Pekanbaru , Riau untuk menjalani istirahat .\" Setahu saya dia orangnya sehat , tapi tahun lalu saya dengar dia sakit .\n",
            "( Karena ) sakitnya , ia langsung pulang ke Pekanbaru , jadi kami yang mau jenguk juga susah .\n",
            "Barangkali mau istirahat , ya betul juga , kalau di Jakarta susah isirahatnya , \" kata Lula kepada CNNIndonesia.com , Jumat ( 4 / 8 ) .Lula yang mengenal Ryan sejak sebelum aktif berkarier di televisi mengaku belum sempat membesuk Ryan lantaran lokasi yang jauh .\n",
            "Dia juga tak tahu penyakit apa yang diderita Ryan .\" Itu saya enggak tahu , belum sempat jenguk dan enggak selamanya bisa dijenguk juga .\n",
            "Enggak tahu berat sekali apa bagaimana , \" tutur Ryan .Walau sudah setahun menderita sakit , Lula tak mengetahui apa penyebab pasti kematian Dr Oz Indonesia itu .\n",
            "Meski demikian , ia mendengar beberapa kabar yang menyebut bahwa penyebab Ryan meninggal adalah karena jatuh di kamar mandi .“ Saya tidak tahu , barangkali penyakit yang dulu sama yang sekarang berbeda , atau penyebab kematiannya beda dari penyakit sebelumnya .\n",
            "Kita kan enggak bisa mengambil kesimpulan , \" kata Lula .Ryan Thamrin terkenal sebagai dokter yang rutin membagikan tips dan informasi kesehatan lewat tayangan Dokter Oz Indonesia .Ryan menempuh Pendidikan Dokter pada tahun 2002 di Fakultas Kedokteran Universitas Gadjah Mada .\n",
            "Dia kemudian melanjutkan pendidikan Klinis Kesehatan Reproduksi dan Penyakit Menular Seksual di Mahachulalongkornrajavidyalaya University , Bangkok , Thailand pada 2004 .\n",
            "Summary : Dokter Lula Kamal yang merupakan selebriti sekaligus rekan kerja Ryan Thamrin menyebut kawannya itu sudah sakit sejak setahun yang lalu .\n",
            "Lula menuturkan , sakit itu membuat Ryan mesti vakum dari semua kegiatannya , termasuk menjadi pembawa acara Dokter Oz Indonesia .\n",
            "Kondisi itu membuat Ryan harus kembali ke kampung halamannya di Pekanbaru , Riau untuk menjalani istirahat .\n",
            "\n",
            "Paragraph : Selfie ialah salah satu tema terpanas di kalangan produsen smartphone , bahkan menjadi senjata andalan beberapa brand terkenal .\n",
            "Anda mungkin berpikir bahwa saat ini , pasar handset spesialis selfie sudah sangat sesak .\n",
            "Tapi Asus masih melihat adanya peluang besar menanti di sana .\n",
            "Dari data mereka , sebanyak 71 persen orang Indonesia setidaknya mengambil selfie atau wefie setiap minggu .Setelah mulai menyelami ranah swafoto dua tahun silam lewat ZanFone Selfie , sang produsen hardware asal Taiwan itu akhirnya membawa sepasang pewarisnya ke tanah air .\n",
            "Handset - handset ini merupakan anggota keluarga ZenFone generasi keempat dan keduanya sama-sama dibekali setup kamera ganda di depan .\n",
            "Mereka adalah Asus ZenFone 4 Selfie Pro ZD552KL dan ZenFone 4 Selfie ZD553KL .CEO Asus Jerry Shen menjelaskan bahwa sudah waktunya bagi Asus untuk memberikan penawaran baru buat penggemar self - portrait demi menunjukkan keseriusan mereka di segmen itu .\n",
            "Walaupun   telah tersedia banyak pilihan ,   Asus berpendapat bahwa konsumen di Indonesia membutuhkan solusi yang ‘ lebih profesional’ .\n",
            "Dua ZenFone Selfie anyar ini kabarnya diracik sedemikian rupa sebagai jawaban atas kekurangan yang ada di perangkat - perangkat kompetitor , khususnya pada aspek jangkauan lensa dan performa di kondisi low-light .ZenFone 4 Selfie Pro   merupakan produk swafoto pamungkas Asus .\n",
            "Tubuh berbahan aluminiumnya dibuat melalui teknik nano molding dan tiap lekukannya dibentuk secara presisi .\n",
            "Dipadu layar berlapis Corning Gorilla Glass 5 2.5D , saya akui penampilan handset ini sangat menawan , terutama   untuk varian berwarna merahnya .\n",
            "Sebagai jendela akses konten , smartphone    menghidangkan layar AMOLED 1080p berkepadatan 401 ppi seluas 5,5 - inci .Atraksi utama dari ZenFone 4 Selfie Pro tentu saja adalah kamera depannya .\n",
            "Di sana , Asus mencantumkan sistem DuoPixel 24Mp , berisi kombinasi sepasang sensor Sony Exmor RS IMX362 1,4 µm 12 - megapixel dengan aperture f/1.8 , ditambah sensor Omnivision 5670 1,12 µm f/2.2 yang memiliki lensa wide-angle 120 derajat , sehingga kamera bisa merangkul objek dua kali lebih banyak – memungkinkan Anda ber-selfie bersama kawan ataupun keluarga tanpa bantuan monopod .Selain itu , Asus melengkapi ZenFone 4 Selfie Pro dengan bundel perkakas khusus swafoto bernama SelfieMaster .\n",
            "Tool ini berisi fitur-fitur krusial semisal beautify ( buat foto maupun video ) , serta kolase dan BeautyLive .\n",
            "Produsen juga tak lupa menyiapkan flash LED softlight   untuk membantu pengambilan foto di kondisi temaram .Kamera belakangnya sendiri mengandalkan sensor Sony Exmor IMX351 1 µm 16 - megapixel berlensa 26 mm f/2.2 .\n",
            "Ia dibantu sistem electronic image stabilization , phase detection autofocus , serta LED dual - tone flash .Di dalam , Asus mempersenjatai ZD552KL dengan chip Qualcomm Snapdragon 625 ( ada prosesor octa-core Cortex - A53 2 GHz dan GPU Adreno 506 ) , RAM sebesar 4 GB , ROM 64 GB , dan baterai 3.000 mAh .\n",
            "Smartphone berjalan di sistem operasi Android 7.1.1 Nougat plus interface ZenUI 4.0 .ZD553KL ialah alternatif lebih terjangkau dari saudarinya di atas .\n",
            "Handset mengusung arahan desain serupa ZenFone 4 Selfie Pro , namun konstruksi tubuhnya terbuat dari plastik , dan layar IPS 5,5 - incinya menyajikan resolusi 720p .\n",
            "Tapi jangan cemas soal penampilannya , smartphone tetap memanfaatkan kaca 2.5D sehingga memberi kesan menyambung pada lekukan di sisi samping .Kapabilitas swafoto ZenFone 4 Selfie bersandar pada sensor Omnivision 20880 1 µm 20 - megapixel f/2.0 dan sensor Omnivision 8856 1,12 µm f/2.4 dengan lensa wide-angle 120 derajat .\n",
            "Sudut jangkauan jepretan dan sejumlah kelengkapannya tak berbeda dari Selfie 4 Pro .\n",
            "Anda kembali dihindangkan SelfieMaster , flash LED softlight , mode panorama , serta HDR .Ukuran megapixel kamera belakangnya setara ZenFone 4 Selfie Pro , namun jenis sensornya berbeda .\n",
            "Smartphone tersebut menggunakan Omnivision 16880 1 µm 16Mp dengan aperture lensa f/2.2 .\n",
            "Meski demikian , fitur-fitur penunjang fotografi seperti PDAF , EIS , dan flash LED juga tetap ada di sana .ZenFone 4 Selfie diotaki system-on-chip Qualcomm Snapdragon 430 , berisi CPU octa-core Cortex - A53 1,4 GHz dan GPU Adreno 505 .\n",
            "Handset menyimpan RAM 4 GB , memori internal 64 GB , lalu tenaganya dipasok oleh baterai 3.000 mAh non - removable di dalam .\n",
            "Satu keunikan yang membuat ZD553KL lebih ‘ unggul’ dari ZenFone 4 Selfie Pro adalah dukungan tiga slot kartu : dua untuk SIM , dan satu lagi buat microSD ( storage dapat diekspansi   sampai   2 TB ) .\n",
            "Smartphone juga beroperasi di platform Android 7.1.1 Nougat dengan ZenUI 4.0 .ZenFone 4 Selfie Pro ZD552KL dibanderol seharga Rp 5 juta , sedangkan ZenFone 4 Selfie ZD553KL dipatok di harga Rp 3,5 juta .\n",
            "Kedua perangkat sudah mulai dipasarkan mulai tanggal 25 November , dan sampai   tanggal 10 November nanti , paket penjualan turut dibundel bersama ‘ Gong Yoo Special Gift Box’ serta speaker Bluetooth khusus ZenFone 4 Selfie Pro – selama persediaan masih ada .DailySocial.id adalah portal berita startup dan inovasi teknologi .\n",
            "Kamu bisa menjadi member komunitas startup dan inovasi DailySocial.id , mengunduh laporan riset dan statistik seputar teknologi secara cuma-cuma , dan mengikuti berita startup Indonesia dan gadget terbaru .\n",
            "Summary : Asus memperkenalkan   ZenFone generasi keempat dan keduanya sama-sama dibekali setup kamera ganda di depan .\n",
            "Mereka adalah Asus ZenFone 4 Selfie Pro ZD552KL dan ZenFone 4 Selfie ZD553KL .\n",
            "Dua ZenFone Selfie anyar ini kabarnya diracik sedemikian rupa sebagai jawaban atas kekurangan yang ada di perangkat - perangkat kompetitor , khususnya pada aspek jangkauan lensa dan performa di kondisi low-light .\n",
            "\n",
            "Paragraph : Jakarta , CNN Indonesia - - Dinas Pariwisata Provinsi Bengkulu kembali menggelar kegiatan Bimbingan Teknis ( Bimtek ) SDM Kepariwisataan dalam menyongson \" Visit 2020 Wonderful Bengkulu \" .Kegiatan yang berlangsung pada 8 hingga 10 November kemarin tersebut sebagai bagian dari upaya Pemerintah Provinsi Bengkulu dalamHadir sebagai pemateri kegiatan pada 8 - 10 November itu adalah Plt.\n",
            "Asdep Strategi Pemasaran Pariwisata Nusantara , Deputi Bidang Pengembangan Pemasaran Pariwisata Nusantara Hariyanto serta perwakilan dari Deputi Bidang Pengembangan Kelembagaan Kementerian Pariwisata , Faizal .Kepala Dinas Pariwisata Provinsi Bengkulu Yudi Satria mengatakan , kegiatan Bimtek diikuti 250 peserta yang terdiri dari aparatur Pemerintah Provinsi , ASN Kabupaten / Kota , Kelompok Sadar Wisata serta pihak terkait sektor pariwisata di Bengkulu .\" Kegiatan ini dimaksudkan untuk memberikan pembekalan kepada peserta di bidang kepariwisataan , \" ujar Yudi Satria .Ia mengatakan , Pemprov telah menetapkan pariwisata sebagai salah satu sektor yang akan dikembangkan dan akan menjadi sektor unggulan dalam meningkatkan pertumbuhan ekonomi daerah serta masyarakat .Hal itu , jelas Yudi , tidak lepas dari potensi pariwisata di Bengkulu yang besar memiliki kekayaan alam yang indah serta budaya yang tinggi .\" Karena itu pula Pemprov telah menetapkan program ' Visit 2020 Wonderful Bengkulu \" yang akan menjadi tujuan besar pariwisata Bengkulu .\n",
            "Salah satu poin utamanya adalah upaya menghasilkan SDM pariwisata yang andal yang akan diwujudkan melalui Bimtek ini , \" ujar Yudi .Selain itu , dalam menunjang proses ' Visit 2020 Wonderful Bengkulu ' , Pemprov juga telah menyiapkan 52 acara yang akan digelar dalam satu tahun ke depan yang bertujuan mengangkat potensi lokal ke kelas dunia .\" Salah satu yang dirancang secara besar adalah Sail Bengkulu yang akan menjual keindahan alam laut Bengkulu yang berhadapan langsung dengan Samudera Hindia , \" kata dia .Ajang lainnya tentunya Festival Tabot Muharam yang selalu menyedot minat ribuan wisatawan setiap tahun serta Festival Bumi Rafflesia .\" Semua kegiatan ini harus disinkronkan , penguatan SDM dan bagaimana promosi pemasarannya agar semua acara ini layak jual untuk wisatawan , \" ujarnya .Sementara Hariyanto dalam kesempatan itu menyampaikan materi tentang pengembangan SDM sektor pariwisata dan pengemasan serta pemasaran satu acara .Dalam paparanya ia menjelaskan , bahwa Bimtek penting untuk meningkatkan kompetensi SDM kepariwisataan sehingga mampu berperan dalam peningkatan pembangunan kepariwisataan .Selain itu ia menegaskan bahwa dalam pengembangan pariwisata , promosi juga hal yang harus diperhatikan .Misalnya penekanan pada mekanisme promosi pariwisata secara digital atau daring , penyelenggaraan acara juga harus terkurasi dengan baik sehingga memiliki daya tarik yang kuat .\" Bagaimana juga meningkatkan efektivitas partisipasi Dinas Pariwisata pada penyelenggaraan acara dan bagaimana cara memasarkan dan mempromosikan agar bisa dikenal dan menjadi daya tarik wisatawan , ” terang Hariyanto .Deputi Pengembangan Pemasaran Pariwisata Nusantara , Esthy Reko Astuti mengatakan , Bimtek kali ini juga bertujuan memberi perspektif dan arah yang sama tentang program promosi Pariwisata di Bengkulu .\" Selain itu juga untuk memahami potensi destinasi - destinasi wisata di Bengkulu , \" ujar Esthy .Menteri Pariwisata Arief Yahya mengapresiasi kegiatan Bimtek sebagai salah satu upaya dan komitmen dari Pemerintah Provinsi Bengkulu dalam mewujudkan pariwisata sebagai salah satu sektor utama .\n",
            "Ia pun berkomitmen akan mendung Penprov dalam mewujudkan \" Visit 2020 Wonderful Bengkulu \" .\" Obyek wisata andalan Bengkulu , Benteng Marlborough , Rumah Bung Karno , Pantai Panjang yang memiliki pasir putih yang indah dan bersih .\n",
            "Semuanya world class , ditambah dengan puluhan atraksi didalamnya , tinggal kita akan dukung dan promosikan ' Bengkulu Visit ' sehingga lebih mendunia , \" kata Arief ( syahb )\n",
            "Summary : Dinas Pariwisata Provinsi Bengkulu kembali menggelar kegiatan Bimbingan Teknis ( Bimtek ) SDM Kepariwisataan dalam menyongson \" Visit 2020 Wonderful Bengkulu \" pada 8 - 10 November 2017 yang lalu .\n",
            "Kegiatan yang berlangsung pada 8 hingga 10 November kemarin tersebut sebagai bagian dari upaya Pemerintah Provinsi Bengkulu dalam memperkuat SDM Pariwisata untuk menyongsong \" Visit 2020 Wonderful Bengkulu \" .\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGLo9wvJTYKF"
      },
      "source": [
        "#data cleaning\n",
        "def clean_data(txt):\n",
        "  #1. Remove punctuations and special characters\n",
        "  txt = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', txt, flags=re.MULTILINE)\n",
        "  txt = re.sub(r'\\<a href', ' ', txt)\n",
        "  txt = re.sub(r'&amp;', '', txt) \n",
        "  txt = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', txt)\n",
        "  txt = re.sub(r'<br />', ' ', txt)\n",
        "  txt = re.sub(r'\\'', ' ', txt)\n",
        "  #2. Convert lower case\n",
        "  txt = txt.lower()\n",
        "\n",
        "  #3. Remove STOPWORDS (sastrawi(?) spacy id (?))\n",
        "  txt = txt.split()\n",
        "  txt = [word for word in txt if not word in stopwords]\n",
        "  txt = ' '.join(txt)\n",
        "\n",
        "  return txt"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFeXANCaXGTC",
        "outputId": "f80f0ef3-87e1-40cb-98f4-e1ed155a844d"
      },
      "source": [
        "clean_summaries = []\n",
        "for summary in tqdm(data.summary):\n",
        "    clean_summaries.append(clean_data(summary))\n",
        "print(\"Summaries are complete.\")\n",
        "\n",
        "clean_texts = []\n",
        "for text in tqdm(data.paragraphs):\n",
        "    clean_texts.append(clean_data(text))\n",
        "print(\"Texts are complete.\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 14262/14262 [00:01<00:00, 10376.34it/s]\n",
            "  2%|▏         | 214/14262 [00:00<00:06, 2131.49it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Summaries are complete.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 14262/14262 [00:06<00:00, 2132.26it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Texts are complete.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXfIP2NKXhq1",
        "outputId": "86e213ab-afe3-491c-a557-18e2a6dc7f39"
      },
      "source": [
        "for i in range(3):\n",
        "  print(\"Paragraph : \" + clean_summaries[i])\n",
        "  print(\"Summary : \" + clean_texts[i] + \"\\n\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Paragraph : dokter lula kamal merupakan selebriti sekaligus rekan kerja ryan thamrin menyebut kawannya sakit sejak setahun lalu lula menuturkan sakit membuat ryan mesti vakum semua kegiatannya termasuk menjadi pembawa acara dokter oz indonesia kondisi membuat ryan kampung halamannya pekanbaru riau menjalani istirahat\n",
            "Summary : jakarta cnn indonesia dokter ryan thamrin terkenal lewat acara dokter oz indonesia meninggal dunia jumat 4 8 dini hari dokter lula kamal merupakan selebriti sekaligus rekan kerja ryan menyebut kawannya sakit sejak setahun lalu lula menuturkan sakit membuat ryan mesti vakum semua kegiatannya termasuk menjadi pembawa acara dokter oz indonesia kondisi membuat ryan kampung halamannya pekanbaru riau menjalani istirahat setahu orangnya sehat tahun lalu dengar sakit sakitnya langsung pulang pekanbaru jadi mau jenguk susah barangkali mau istirahat betul kalau jakarta susah isirahatnya kata lula cnnindonesia com jumat 4 8 lula mengenal ryan sejak aktif berkarier televisi mengaku sempat membesuk ryan lantaran lokasi jauh tak tahu penyakit apa diderita ryan enggak tahu sempat jenguk enggak selamanya dijenguk enggak tahu berat sekali apa bagaimana tutur ryan setahun menderita sakit lula tak mengetahui apa penyebab kematian dr oz indonesia meski mendengar beberapa kabar menyebut penyebab ryan meninggal jatuh kamar mandi “ tahu barangkali penyakit dulu sama sekarang berbeda penyebab kematiannya beda penyakit sebelumnya kan enggak mengambil kesimpulan kata lula ryan thamrin terkenal dokter rutin membagikan tips informasi kesehatan lewat tayangan dokter oz indonesia ryan menempuh pendidikan dokter tahun 2002 fakultas kedokteran universitas gadjah mada kemudian melanjutkan pendidikan klinis kesehatan reproduksi penyakit menular seksual mahachulalongkornrajavidyalaya university bangkok thailand 2004\n",
            "\n",
            "Paragraph : asus memperkenalkan zenfone generasi keempat keduanya sama sama dibekali setup kamera ganda depan asus zenfone 4 selfie pro zd552kl zenfone 4 selfie zd553kl zenfone selfie anyar kabarnya diracik sedemikian rupa jawaban atas kekurangan perangkat perangkat kompetitor khususnya aspek jangkauan lensa performa kondisi low light\n",
            "Summary : selfie ialah salah satu tema terpanas kalangan produsen smartphone bahkan menjadi senjata andalan beberapa brand terkenal mungkin berpikir pasar handset spesialis selfie sangat sesak asus melihat adanya peluang besar menanti sana data sebanyak 71 persen orang indonesia mengambil selfie wefie minggu mulai menyelami ranah swafoto tahun silam lewat zanfone selfie sang produsen hardware asal taiwan akhirnya membawa sepasang pewarisnya tanah air handset handset merupakan anggota keluarga zenfone generasi keempat keduanya sama sama dibekali setup kamera ganda depan asus zenfone 4 selfie pro zd552kl zenfone 4 selfie zd553kl ceo asus jerry shen menjelaskan waktunya asus memberikan penawaran baru buat penggemar self portrait menunjukkan keseriusan segmen walaupun tersedia banyak pilihan asus berpendapat konsumen indonesia membutuhkan solusi ‘ lebih profesional’ zenfone selfie anyar kabarnya diracik sedemikian rupa jawaban atas kekurangan perangkat perangkat kompetitor khususnya aspek jangkauan lensa performa kondisi low light zenfone 4 selfie pro merupakan produk swafoto pamungkas asus tubuh berbahan aluminiumnya dibuat melalui teknik nano molding tiap lekukannya dibentuk presisi dipadu layar berlapis corning gorilla glass 5 2 5d akui penampilan handset sangat menawan terutama varian berwarna merahnya jendela akses konten smartphone menghidangkan layar amoled 1080p berkepadatan 401 ppi seluas 5 5 inci atraksi utama zenfone 4 selfie pro kamera depannya sana asus mencantumkan sistem duopixel 24mp berisi kombinasi sepasang sensor sony exmor rs imx362 1 4 µm 12 megapixel aperture f 1 8 ditambah sensor omnivision 5670 1 12 µm f 2 2 memiliki lensa wide angle 120 derajat kamera merangkul objek kali lebih banyak – memungkinkan ber selfie bersama kawan ataupun keluarga bantuan monopod asus melengkapi zenfone 4 selfie pro bundel perkakas khusus swafoto bernama selfiemaster tool berisi fitur fitur krusial semisal beautify buat foto maupun video kolase beautylive produsen tak lupa menyiapkan flash led softlight membantu pengambilan foto kondisi temaram kamera belakangnya sendiri mengandalkan sensor sony exmor imx351 1 µm 16 megapixel berlensa 26 mm f 2 2 dibantu sistem electronic image stabilization phase detection autofocus led dual tone flash asus mempersenjatai zd552kl chip qualcomm snapdragon 625 prosesor octa core cortex a53 2 ghz gpu adreno 506 ram sebesar 4 gb rom 64 gb baterai 3 000 mah smartphone berjalan sistem operasi android 7 1 1 nougat plus interface zenui 4 0 zd553kl ialah alternatif lebih terjangkau saudarinya atas handset mengusung arahan desain serupa zenfone 4 selfie pro konstruksi tubuhnya terbuat plastik layar ips 5 5 incinya menyajikan resolusi 720p jangan cemas soal penampilannya smartphone tetap memanfaatkan kaca 2 5d memberi kesan menyambung lekukan sisi samping kapabilitas swafoto zenfone 4 selfie bersandar sensor omnivision 20880 1 µm 20 megapixel f 2 0 sensor omnivision 8856 1 12 µm f 2 4 lensa wide angle 120 derajat sudut jangkauan jepretan sejumlah kelengkapannya tak berbeda selfie 4 pro dihindangkan selfiemaster flash led softlight mode panorama hdr ukuran megapixel kamera belakangnya setara zenfone 4 selfie pro jenis sensornya berbeda smartphone tersebut menggunakan omnivision 16880 1 µm 16mp aperture lensa f 2 2 meski fitur fitur penunjang fotografi pdaf eis flash led tetap sana zenfone 4 selfie diotaki system on chip qualcomm snapdragon 430 berisi cpu octa core cortex a53 1 4 ghz gpu adreno 505 handset menyimpan ram 4 gb memori internal 64 gb lalu tenaganya dipasok baterai 3 000 mah non removable satu keunikan membuat zd553kl lebih ‘ unggul’ zenfone 4 selfie pro dukungan tiga slot kartu sim satu buat microsd storage diekspansi 2 tb smartphone beroperasi platform android 7 1 1 nougat zenui 4 0 zenfone 4 selfie pro zd552kl dibanderol seharga rp 5 juta zenfone 4 selfie zd553kl dipatok harga rp 3 5 juta kedua perangkat mulai dipasarkan mulai tanggal 25 november tanggal 10 november paket penjualan turut dibundel bersama ‘ gong yoo special gift box’ speaker bluetooth khusus zenfone 4 selfie pro – selama persediaan dailysocial id portal berita startup inovasi teknologi kamu menjadi member komunitas startup inovasi dailysocial id mengunduh laporan riset statistik seputar teknologi cuma cuma mengikuti berita startup indonesia gadget terbaru\n",
            "\n",
            "Paragraph : dinas pariwisata provinsi bengkulu menggelar kegiatan bimbingan teknis bimtek sdm kepariwisataan menyongson visit 2020 wonderful bengkulu 8 10 november 2017 lalu kegiatan berlangsung 8 hingga 10 november kemarin tersebut bagian upaya pemerintah provinsi bengkulu memperkuat sdm pariwisata menyongsong visit 2020 wonderful bengkulu\n",
            "Summary : jakarta cnn indonesia dinas pariwisata provinsi bengkulu menggelar kegiatan bimbingan teknis bimtek sdm kepariwisataan menyongson visit 2020 wonderful bengkulu kegiatan berlangsung 8 hingga 10 november kemarin tersebut bagian upaya pemerintah provinsi bengkulu dalamhadir pemateri kegiatan 8 10 november plt asdep strategi pemasaran pariwisata nusantara deputi bidang pengembangan pemasaran pariwisata nusantara hariyanto perwakilan deputi bidang pengembangan kelembagaan kementerian pariwisata faizal kepala dinas pariwisata provinsi bengkulu yudi satria mengatakan kegiatan bimtek diikuti 250 peserta terdiri aparatur pemerintah provinsi asn kabupaten kota kelompok sadar wisata pihak terkait sektor pariwisata bengkulu kegiatan dimaksudkan memberikan pembekalan peserta bidang kepariwisataan ujar yudi satria mengatakan pemprov menetapkan pariwisata salah satu sektor dikembangkan menjadi sektor unggulan meningkatkan pertumbuhan ekonomi daerah masyarakat jelas yudi lepas potensi pariwisata bengkulu besar memiliki kekayaan alam indah budaya tinggi pemprov menetapkan program visit 2020 wonderful bengkulu menjadi tujuan besar pariwisata bengkulu salah satu poin utamanya upaya menghasilkan sdm pariwisata andal diwujudkan melalui bimtek ujar yudi menunjang proses visit 2020 wonderful bengkulu pemprov menyiapkan 52 acara digelar satu tahun depan bertujuan mengangkat potensi lokal kelas dunia salah satu dirancang besar sail bengkulu menjual keindahan alam laut bengkulu berhadapan langsung samudera hindia kata ajang lainnya tentunya festival tabot muharam selalu menyedot minat ribuan wisatawan tahun festival bumi rafflesia semua kegiatan disinkronkan penguatan sdm bagaimana promosi pemasarannya semua acara layak jual wisatawan ujarnya hariyanto kesempatan menyampaikan materi pengembangan sdm sektor pariwisata pengemasan pemasaran satu acara paparanya menjelaskan bimtek penting meningkatkan kompetensi sdm kepariwisataan mampu berperan peningkatan pembangunan kepariwisataan menegaskan pengembangan pariwisata promosi diperhatikan misalnya penekanan mekanisme promosi pariwisata digital daring penyelenggaraan acara terkurasi baik memiliki daya tarik kuat bagaimana meningkatkan efektivitas partisipasi dinas pariwisata penyelenggaraan acara bagaimana cara memasarkan mempromosikan dikenal menjadi daya tarik wisatawan ” terang hariyanto deputi pengembangan pemasaran pariwisata nusantara esthy reko astuti mengatakan bimtek kali bertujuan memberi perspektif arah sama program promosi pariwisata bengkulu memahami potensi destinasi destinasi wisata bengkulu ujar esthy menteri pariwisata arief yahya mengapresiasi kegiatan bimtek salah satu upaya komitmen pemerintah provinsi bengkulu mewujudkan pariwisata salah satu sektor utama berkomitmen mendung penprov mewujudkan visit 2020 wonderful bengkulu obyek wisata andalan bengkulu benteng marlborough rumah bung karno pantai panjang memiliki pasir putih indah bersih semuanya world class ditambah puluhan atraksi didalamnya tinggal dukung promosikan bengkulu visit lebih mendunia kata arief syahb\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b87oQBqpXrn_",
        "outputId": "c2014246-9a66-4c75-9f0a-f65e5a1a465c"
      },
      "source": [
        "def count_words(count_dict, text):\n",
        "    '''Count the number of occurrences of each word in a set of text'''\n",
        "    for sentence in text:\n",
        "        for word in sentence.split():\n",
        "            if word not in count_dict:\n",
        "                count_dict[word] = 1\n",
        "            else:\n",
        "                count_dict[word] += 1\n",
        "\n",
        "word_counts = {}\n",
        "\n",
        "count_words(word_counts, clean_summaries)\n",
        "count_words(word_counts, clean_texts)\n",
        "            \n",
        "print(\"Size of Vocabulary:\", len(word_counts))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of Vocabulary: 99764\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibjCI1iecZI_"
      },
      "source": [
        "###Word Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fo4SAb1XcBat"
      },
      "source": [
        "#using word2vec for indonesian trained with Wikipedia Indonesia datasets https://github.com/deryrahman/word2vec-bahasa-indonesia\n",
        "# import gensim\n",
        "\n",
        "# w2v_path = '/content/drive/MyDrive/idwiki_word2vec_100/idwiki_word2vec_100.model'\n",
        "# model = gensim.models.word2vec.Word2Vec.load(w2v_path)\n",
        "# model.wv.save_word2vec_format('test_w2v.txt', binary=False)\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cp7MkEJnwOG6",
        "outputId": "37db8c00-2b59-455d-a691-f2df395da415"
      },
      "source": [
        "embeddings_index = {}\n",
        "with open('/content/drive/MyDrive/idwiki_word2vec_100/idwiki_word2vec.txt', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split(' ')\n",
        "        word = values[0]\n",
        "        embedding = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = embedding\n",
        "\n",
        "print('Word embeddings:', len(embeddings_index))\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word embeddings: 331793\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSIU8jZRbb5r",
        "outputId": "6ce43a1d-3cfd-4808-fb4e-c5ec120573b5"
      },
      "source": [
        "missing_words = 0\n",
        "threshold = 20\n",
        "\n",
        "for word, count in word_counts.items():\n",
        "    if count > threshold:\n",
        "        if word not in embeddings_index:\n",
        "            missing_words += 1\n",
        "            \n",
        "missing_ratio = round(missing_words/len(word_counts),4)*100\n",
        "            \n",
        "print(\"Number of words missing from word2vec:\", missing_words)\n",
        "print(\"Percent of words that are missing from vocabulary: {}%\".format(missing_ratio))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of words missing from word2vec: 613\n",
            "Percent of words that are missing from vocabulary: 0.61%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faplQoKobj5K",
        "outputId": "6732d4ee-ac92-40ed-9592-a5e8ff27f3f6"
      },
      "source": [
        "# Limit the vocab that we will use to words that appear ≥ threshold or are in GloVe\n",
        "\n",
        "#dictionary to convert words to integers\n",
        "vocab_to_int = {} \n",
        "\n",
        "value = 0\n",
        "for word, count in word_counts.items():\n",
        "    if count >= threshold or word in embeddings_index:\n",
        "        vocab_to_int[word] = value\n",
        "        value += 1\n",
        "\n",
        "# Special tokens that will be added to our vocab\n",
        "codes = [\"<UNK>\",\"<PAD>\",\"<EOS>\",\"<GO>\"]   \n",
        "\n",
        "# Add codes to vocab\n",
        "for code in codes:\n",
        "    vocab_to_int[code] = len(vocab_to_int)\n",
        "\n",
        "# Dictionary to convert integers to words\n",
        "int_to_vocab = {}\n",
        "for word, value in vocab_to_int.items():\n",
        "    int_to_vocab[value] = word\n",
        "\n",
        "usage_ratio = round(len(vocab_to_int) / len(word_counts),4)*100\n",
        "\n",
        "print(\"Total number of unique words:\", len(word_counts))\n",
        "print(\"Number of words we will use:\", len(vocab_to_int))\n",
        "print(\"Percent of words we will use: {}%\".format(usage_ratio))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of unique words: 99764\n",
            "Number of words we will use: 64666\n",
            "Percent of words we will use: 64.82%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHWt2836d9OK",
        "outputId": "c21a9ec9-914f-477f-eb87-96c5e16b2416"
      },
      "source": [
        "embedding_dim = 100 # Need to use 100 for embedding dimensions to match word2vec vectors.\n",
        "nb_words = len(vocab_to_int)\n",
        "\n",
        "# Create matrix with default values of zero\n",
        "word_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)\n",
        "for word, i in vocab_to_int.items():\n",
        "    if word in embeddings_index:\n",
        "        word_embedding_matrix[i] = embeddings_index[word]\n",
        "    else:\n",
        "        # If word not in CN, create a random embedding for it\n",
        "        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
        "        embeddings_index[word] = new_embedding\n",
        "        word_embedding_matrix[i] = new_embedding\n",
        "\n",
        "# Check if value matches len(vocab_to_int)\n",
        "print(len(word_embedding_matrix))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "64666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xE7sW1fofCwM"
      },
      "source": [
        "def convert_to_ints(text, word_count, unk_count, eos=False):\n",
        "    '''Convert words in text to an integer.\n",
        "       If word is not in vocab_to_int, use UNK's integer.\n",
        "       Total the number of words and UNKs.\n",
        "       Add EOS token to the end of texts'''\n",
        "    ints = []\n",
        "    for sentence in text:\n",
        "        sentence_ints = []\n",
        "        for word in sentence.split():\n",
        "            word_count += 1\n",
        "            if word in vocab_to_int:\n",
        "                sentence_ints.append(vocab_to_int[word])\n",
        "            else:\n",
        "                sentence_ints.append(vocab_to_int[\"<UNK>\"])\n",
        "                unk_count += 1\n",
        "        if eos:\n",
        "            sentence_ints.append(vocab_to_int[\"<EOS>\"])\n",
        "        ints.append(sentence_ints)\n",
        "    return ints, word_count, unk_count"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQkPfzEfhkC7",
        "outputId": "7cb1b812-d1ef-45fa-d770-9a12224b67b0"
      },
      "source": [
        "# Apply convert_to_ints to clean_summaries and clean_texts\n",
        "word_count = 0\n",
        "unk_count = 0\n",
        "\n",
        "int_summaries, word_count, unk_count = convert_to_ints(clean_summaries, word_count, unk_count)\n",
        "int_texts, word_count, unk_count = convert_to_ints(clean_texts, word_count, unk_count, eos=True)\n",
        "\n",
        "unk_percent = round(unk_count/word_count,4)*100\n",
        "\n",
        "print(\"Total number of words in headlines:\", word_count)\n",
        "print(\"Total number of UNKs in headlines:\", unk_count)\n",
        "print(\"Percent of words that are UNK: {}%\".format(unk_percent))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of words in headlines: 3844551\n",
            "Total number of UNKs in headlines: 69971\n",
            "Percent of words that are UNK: 1.82%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81zOFc1giUno"
      },
      "source": [
        "def create_lengths(text):\n",
        "    '''Create a data frame of the sentence lengths from a text'''\n",
        "    lengths = []\n",
        "    for sentence in text:\n",
        "        lengths.append(len(sentence))\n",
        "    return pd.DataFrame(lengths, columns=['counts'])"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6z33jcAOikKq",
        "outputId": "e3913a39-6ab8-4260-c52c-cbfbc3d22a27"
      },
      "source": [
        "lengths_summaries = create_lengths(int_summaries)\n",
        "lengths_texts = create_lengths(int_texts)\n",
        "\n",
        "print(\"Summaries:\")\n",
        "print(lengths_summaries.describe())\n",
        "print()\n",
        "print(\"Texts:\")\n",
        "print(lengths_texts.describe())"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Summaries:\n",
            "             counts\n",
            "count  14262.000000\n",
            "mean      46.488992\n",
            "std        4.929286\n",
            "min       26.000000\n",
            "25%       43.000000\n",
            "50%       46.000000\n",
            "75%       50.000000\n",
            "max       74.000000\n",
            "\n",
            "Texts:\n",
            "             counts\n",
            "count  14262.000000\n",
            "mean     224.077058\n",
            "std       88.483104\n",
            "min       26.000000\n",
            "25%      164.000000\n",
            "50%      209.000000\n",
            "75%      267.000000\n",
            "max      929.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpjMT_gpi3M2",
        "outputId": "35ed2fc3-294a-4643-b3ce-c2277c9d1073"
      },
      "source": [
        "# Inspect the length of texts\n",
        "print(np.percentile(lengths_texts.counts, 90))\n",
        "print(np.percentile(lengths_texts.counts, 95))\n",
        "print(np.percentile(lengths_texts.counts, 99))\n",
        "\n",
        "# Inspect the length of summaries\n",
        "print(np.percentile(lengths_summaries.counts, 90))\n",
        "print(np.percentile(lengths_summaries.counts, 95))\n",
        "print(np.percentile(lengths_summaries.counts, 99))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "336.0\n",
            "387.0\n",
            "516.0\n",
            "53.0\n",
            "55.0\n",
            "58.38999999999942\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fm2hC_DIjHrM"
      },
      "source": [
        "def unk_counter(sentence):\n",
        "    unk_count = 0\n",
        "    for word in sentence:\n",
        "        if word == vocab_to_int[\"<UNK>\"]:\n",
        "            unk_count += 1\n",
        "    return unk_count"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6tTBXfrjZOu",
        "outputId": "73d8dabd-cfe0-4e6b-a32c-48e0777afcd5"
      },
      "source": [
        "sorted_summaries = []\n",
        "sorted_texts = []\n",
        "max_text_length = 334\n",
        "max_summary_length = 58\n",
        "min_length = 2\n",
        "unk_text_limit = 1\n",
        "unk_summary_limit = 0\n",
        "\n",
        "for length in tqdm(range(min(lengths_texts.counts), max_text_length)): \n",
        "    for count, words in enumerate(int_summaries):\n",
        "        if (len(int_summaries[count]) >= min_length and\n",
        "            len(int_summaries[count]) <= max_summary_length and\n",
        "            len(int_texts[count]) >= min_length and\n",
        "            unk_counter(int_summaries[count]) <= unk_summary_limit and\n",
        "            unk_counter(int_texts[count]) <= unk_text_limit and\n",
        "            length == len(int_texts[count])\n",
        "           ):\n",
        "            sorted_summaries.append(int_summaries[count])\n",
        "            sorted_texts.append(int_texts[count])\n",
        "        \n",
        "# Compare lengths to ensure they match\n",
        "print(len(sorted_summaries))\n",
        "print(len(sorted_texts))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 308/308 [01:00<00:00,  5.07it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "3783\n",
            "3783\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2U8ebeL4rbbO"
      },
      "source": [
        "### Building the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7L3rXHG9rZk1"
      },
      "source": [
        "def model_inputs():\n",
        "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
        "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
        "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
        "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
        "    summary_length = tf.placeholder(tf.int32, (None,), name='summary_length')\n",
        "    max_summary_length = tf.reduce_max(summary_length, name='max_dec_len')\n",
        "    text_length = tf.placeholder(tf.int32, (None,), name='text_length')\n",
        "\n",
        "    return input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAEq0cZQTLRh"
      },
      "source": [
        "def process_encoding_input(target_data, vocab_to_int, batch_size):\n",
        "    '''Remove the last word id from each batch and concat the <GO> to the begining of each batch'''\n",
        "    \n",
        "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
        "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
        "\n",
        "    return dec_input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZK6z0S3jTMtd"
      },
      "source": [
        "def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob):\n",
        "    '''Create the encoding layer'''\n",
        "    \n",
        "    for layer in range(num_layers):\n",
        "        with tf.variable_scope('encoder_{}'.format(layer)):\n",
        "            cell_fw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
        "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
        "            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, \n",
        "                                                    input_keep_prob = keep_prob)\n",
        "\n",
        "            cell_bw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
        "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
        "            cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, \n",
        "                                                    input_keep_prob = keep_prob)\n",
        "\n",
        "            enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n",
        "                                                                    cell_bw, \n",
        "                                                                    rnn_inputs,\n",
        "                                                                    sequence_length,\n",
        "                                                                    dtype=tf.float32)\n",
        "    # Join outputs since we are using a bidirectional RNN\n",
        "    enc_output = tf.concat(enc_output,2)\n",
        "    \n",
        "    return enc_output, enc_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7E_Rcm3TNpd"
      },
      "source": [
        "def training_decoding_layer(dec_embed_input, summary_length, dec_cell, initial_state, output_layer, \n",
        "                            vocab_size, max_summary_length):\n",
        "    '''Create the training logits'''\n",
        "    \n",
        "    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
        "                                                        sequence_length=summary_length,\n",
        "                                                        time_major=False)\n",
        "\n",
        "    training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
        "                                                       training_helper,\n",
        "                                                       initial_state,\n",
        "                                                       output_layer) \n",
        "\n",
        "    training_logits, _ , _ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
        "                                                           output_time_major=False,\n",
        "                                                           impute_finished=True,\n",
        "                                                           maximum_iterations=max_summary_length)\n",
        "    return training_decoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTjfwL-KTOU9"
      },
      "source": [
        "def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, initial_state, output_layer,\n",
        "                             max_summary_length, batch_size):\n",
        "    '''Create the inference logits'''\n",
        "    \n",
        "    start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n",
        "    \n",
        "    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
        "                                                                start_tokens,\n",
        "                                                                end_token)\n",
        "                \n",
        "    inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
        "                                                        inference_helper,\n",
        "                                                        initial_state,\n",
        "                                                        output_layer)\n",
        "                \n",
        "    inference_logits, _ , _ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
        "                                                            output_time_major=False,\n",
        "                                                            impute_finished=True,\n",
        "                                                            maximum_iterations=max_summary_length)\n",
        "    \n",
        "    return inference_decoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLDGVdOETOd8"
      },
      "source": [
        "def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, text_length, summary_length, \n",
        "                   max_summary_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers):\n",
        "    '''Create the decoding cell and attention for the training and inference decoding layers'''\n",
        "    \n",
        "    for layer in range(num_layers):\n",
        "        with tf.variable_scope('decoder_{}'.format(layer)):\n",
        "            lstm = tf.contrib.rnn.LSTMCell(rnn_size,\n",
        "                                           initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
        "            dec_cell = tf.contrib.rnn.DropoutWrapper(lstm, \n",
        "                                                     input_keep_prob = keep_prob)\n",
        "    \n",
        "    output_layer = Dense(vocab_size,\n",
        "                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
        "    \n",
        "    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n",
        "                                                  enc_output,\n",
        "                                                  text_length,\n",
        "                                                  normalize=False,\n",
        "                                                  name='BahdanauAttention')\n",
        "\n",
        "    dec_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell,\n",
        "                                                          attn_mech,\n",
        "                                                          rnn_size)\n",
        "            \n",
        "    #initial_state = tf.contrib.seq2seq.AttentionWrapperState(enc_state[0],\n",
        "    #                                                                _zero_state_tensors(rnn_size, \n",
        "    #                                                                                    batch_size, \n",
        "    #                                                                                    tf.float32)) \n",
        "    initial_state = dec_cell.zero_state(batch_size=batch_size,dtype=tf.float32).clone(cell_state=enc_state[0])\n",
        "\n",
        "    with tf.variable_scope(\"decode\"):\n",
        "        training_decoder = training_decoding_layer(dec_embed_input, \n",
        "                                                  summary_length, \n",
        "                                                  dec_cell, \n",
        "                                                  initial_state,\n",
        "                                                  output_layer,\n",
        "                                                  vocab_size, \n",
        "                                                  max_summary_length)\n",
        "        \n",
        "        training_logits,_ ,_ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
        "                                  output_time_major=False,\n",
        "                                  impute_finished=True,\n",
        "                                  maximum_iterations=max_summary_length)\n",
        "    with tf.variable_scope(\"decode\", reuse=True):\n",
        "        inference_decoder = inference_decoding_layer(embeddings,  \n",
        "                                                    vocab_to_int['<GO>'], \n",
        "                                                    vocab_to_int['<EOS>'],\n",
        "                                                    dec_cell, \n",
        "                                                    initial_state, \n",
        "                                                    output_layer,\n",
        "                                                    max_summary_length,\n",
        "                                                    batch_size)\n",
        "        \n",
        "        inference_logits,_ ,_ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
        "                                  output_time_major=False,\n",
        "                                  impute_finished=True,\n",
        "                                  maximum_iterations=max_summary_length)\n",
        "\n",
        "    return training_logits, inference_logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPpgtTrGkEmt"
      },
      "source": [
        "def seq2seq_model(input_data, target_data, keep_prob, text_length, summary_length, max_summary_length, \n",
        "                  vocab_size, rnn_size, num_layers, vocab_to_int, batch_size):\n",
        "    '''Use the previous functions to create the training and inference logits'''\n",
        "    \n",
        "    # Use Numberbatch's embeddings and the newly created ones as our embeddings\n",
        "    embeddings = word_embedding_matrix\n",
        "    \n",
        "    enc_embed_input = tf.nn.embedding_lookup(embeddings, input_data)\n",
        "    enc_output, enc_state = encoding_layer(rnn_size, text_length, num_layers, enc_embed_input, keep_prob)\n",
        "    \n",
        "    dec_input = process_encoding_input(target_data, vocab_to_int, batch_size)\n",
        "    dec_embed_input = tf.nn.embedding_lookup(embeddings, dec_input)\n",
        "    \n",
        "    training_logits, inference_logits  = decoding_layer(dec_embed_input, \n",
        "                                                        embeddings,\n",
        "                                                        enc_output,\n",
        "                                                        enc_state, \n",
        "                                                        vocab_size, \n",
        "                                                        text_length, \n",
        "                                                        summary_length, \n",
        "                                                        max_summary_length,\n",
        "                                                        rnn_size, \n",
        "                                                        vocab_to_int, \n",
        "                                                        keep_prob, \n",
        "                                                        batch_size,\n",
        "                                                        num_layers)\n",
        "    \n",
        "    return training_logits, inference_logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhMDqr4D7baZ",
        "cellView": "form"
      },
      "source": [
        "#@title original seq2seqmodel\n",
        "\n",
        "# def seq2seq_model(input_data, target_data, keep_prob, text_length, summary_length, max_summary_length, \n",
        "#                   vocab_size, rnn_size, num_layers, vocab_to_int, batch_size):\n",
        "    \n",
        "    \n",
        "#     embeddings = word_embedding_matrix\n",
        "    \n",
        "#     enc_embed_input = tf.nn.embedding_lookup(embeddings, input_data)\n",
        "\n",
        "#     #encoding layer\n",
        "#     for layer in range(num_layers):\n",
        "#       with tf.compat.v1.variable_scope('encoder_{}'.format(layer)):\n",
        "#         cell_fw = tf.keras.layers.LSTMCell(rnn_size)\n",
        "#                                               #initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
        "#         cell_fw = tf.nn.RNNCellDropoutWrapper(cell_fw, \n",
        "#                                                     input_keep_prob = keep_prob)\n",
        "\n",
        "#         cell_bw = tf.keras.layers.LSTMCell(rnn_size)\n",
        "#                                               #initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
        "#         cell_bw = tf.nn.RNNCellDropoutWrapper(cell_bw, \n",
        "#                                                     input_keep_prob = keep_prob)\n",
        "\n",
        "#         enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n",
        "#                                                                     cell_bw, \n",
        "#                                                                     enc_embed_input,\n",
        "#                                                                     text_length,\n",
        "#                                                                     dtype=tf.float32)\n",
        "#     # Join outputs since we are using a bidirectional RNN\n",
        "#     enc_output = tf.concat(enc_output,2)\n",
        "#     # enc_output, enc_state = encoding_layer(rnn_size, text_length, num_layers, enc_embed_input, keep_prob)\n",
        "    \n",
        "#     #process encoding input\n",
        "#     ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
        "#     dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
        "\n",
        "#     dec_embed_input = tf.nn.embedding_lookup(embeddings, dec_input)\n",
        "    \n",
        "#     #decoding layer\n",
        "        \n",
        "#     for layer in range(num_layers):\n",
        "#         with tf.compat.v1.variable_scope('decoder_{}'.format(layer)):\n",
        "#             lstm = tf.keras.layers.LSTMCell(rnn_size)\n",
        "#                                            #initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
        "#             dec_cell = tf.nn.RNNCellDropoutWrapper(lstm, \n",
        "#                                                      input_keep_prob = keep_prob)\n",
        "    \n",
        "#     output_layer = Dense(vocab_size,\n",
        "#                          kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
        "    \n",
        "#     attn_mech = tfa.seq2seq.BahdanauAttention(rnn_size,\n",
        "#                                                   enc_output,\n",
        "#                                                   text_length,\n",
        "#                                                   normalize=False,\n",
        "#                                                   name='BahdanauAttention')\n",
        "\n",
        "#     dec_cell = tfa.seq2seq.AttentionWrapper(dec_cell,\n",
        "#                                                           attn_mech,\n",
        "#                                                           rnn_size)\n",
        "            \n",
        "#     #initial_state = tf.contrib.seq2seq.AttentionWrapperState(enc_state[0],\n",
        "#     #                                                                _zero_state_tensors(rnn_size, \n",
        "#     #                                                                                    batch_size, \n",
        "#     #                                                                                    tf.float32)) \n",
        "#     initial_state = dec_cell.zero_state(batch_size=batch_size,dtype=tf.float32).clone(cell_state=enc_state[0])\n",
        "\n",
        "#     with tf.compat.v1.variable_scope(\"decode\"):\n",
        "#         train_sampler = tfa.seq2seq.TrainingSampler(time_major=False)\n",
        "#         training_helper = sampler.initialize(inputs=dec_embed_input,\n",
        "#                                                         text_length=summary_length) \n",
        "      \n",
        "#         training_decoder = tfa.seq2seq.BasicDecoder(dec_cell,\n",
        "#                                                        training_helper,\n",
        "#                                                        initial_state,\n",
        "#                                                        output_layer) \n",
        "      \n",
        "#         training_logits, _ , _ = tfa.seq2seq.dynamic_decode(training_decoder,\n",
        "#                                                            output_time_major=False,\n",
        "#                                                            impute_finished=True,\n",
        "#                                                            maximum_iterations=max_summary_length)\n",
        "        \n",
        "#         # training_decoder = training_decoding_layer(dec_embed_input, \n",
        "#         #                                           summary_length, \n",
        "#         #                                           dec_cell, \n",
        "#         #                                           initial_state,\n",
        "#         #                                           output_layer,\n",
        "#         #                                           vocab_size, \n",
        "#         #                                           max_summary_length)\n",
        "        \n",
        "#         training_logits,_ ,_ = tfa.seq2seq.dynamic_decode(training_decoder,\n",
        "#                                   output_time_major=False,\n",
        "#                                   impute_finished=True,\n",
        "#                                   maximum_iterations=max_summary_length)\n",
        "        \n",
        "#     with tf.compat.v1.variable_scope(\"decode\", reuse=True):\n",
        "            \n",
        "#         start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n",
        "    \n",
        "#         inf_sampler = tfa.seq2seq.GreedyEmbeddingSampler(embeddings)\n",
        "#         inference_helper = inf_sampler.initialize(start_tokens, end_token)\n",
        "                \n",
        "#         inference_decoder = tfa.seq2seq.BasicDecoder(dec_cell,\n",
        "#                                                         inference_helper,\n",
        "#                                                         initial_state,\n",
        "#                                                         output_layer)\n",
        "                \n",
        "#         inference_logits, _ , _ = tfa.seq2seq.dynamic_decode(inference_decoder,\n",
        "#                                                             output_time_major=False,\n",
        "#                                                             impute_finished=True,\n",
        "#                                                             maximum_iterations=max_summary_length)\n",
        "#         # inference_decoder = inference_decoding_layer(embeddings,  \n",
        "#         #                                             vocab_to_int['<GO>'], \n",
        "#         #                                             vocab_to_int['<EOS>'],\n",
        "#         #                                             dec_cell, \n",
        "#         #                                             initial_state, \n",
        "#         #                                             output_layer,\n",
        "#         #                                             max_summary_length,\n",
        "#         #                                             batch_size)\n",
        "        \n",
        "#         inference_logits,_ ,_ = tfa.seq2seq.dynamic_decode(inference_decoder,\n",
        "#                                   output_time_major=False,\n",
        "#                                   impute_finished=True,\n",
        "#                                   maximum_iterations=max_summary_length)\n",
        "#     # training_logits, inference_logits  = decoding_layer(dec_embed_input, \n",
        "#     #                                                     embeddings,\n",
        "#     #                                                     enc_output,\n",
        "#     #                                                     enc_state, \n",
        "#     #                                                     vocab_size, \n",
        "#     #                                                     text_length, \n",
        "#     #                                                     summary_length, \n",
        "#     #                                                     max_summary_length,\n",
        "#     #                                                     rnn_size, \n",
        "#     #                                                     vocab_to_int, \n",
        "#     #                                                     keep_prob, \n",
        "#     #                                                     batch_size,\n",
        "#     #                                                     num_layers)\n",
        "    \n",
        "#     return training_logits, inference_logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBfsT78Qt1eC"
      },
      "source": [
        "def pad_sentence_batch(sentence_batch):\n",
        "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
        "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
        "    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7jjB_mZvMxv"
      },
      "source": [
        "def get_batches(summaries, texts, batch_size):\n",
        "    \"\"\"Batch summaries, texts, and the lengths of their sentences together\"\"\"\n",
        "    for batch_i in range(0, len(texts)//batch_size):\n",
        "        start_i = batch_i * batch_size\n",
        "        summaries_batch = summaries[start_i:start_i + batch_size]\n",
        "        texts_batch = texts[start_i:start_i + batch_size]\n",
        "        pad_summaries_batch = np.array(pad_sentence_batch(summaries_batch))\n",
        "        pad_texts_batch = np.array(pad_sentence_batch(texts_batch))\n",
        "        \n",
        "        # Need the lengths for the _lengths parameters\n",
        "        pad_summaries_lengths = []\n",
        "        for summary in pad_summaries_batch:\n",
        "            pad_summaries_lengths.append(len(summary))\n",
        "        \n",
        "        pad_texts_lengths = []\n",
        "        for text in pad_texts_batch:\n",
        "            pad_texts_lengths.append(len(text))\n",
        "        \n",
        "        yield pad_summaries_batch, pad_texts_batch, pad_summaries_lengths, pad_texts_lengths"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkLUNS6rwTZ4"
      },
      "source": [
        "###Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuIFq7oOwFSg"
      },
      "source": [
        "epochs = 100\n",
        "batch_size = 64\n",
        "rnn_size = 256\n",
        "num_layers = 3\n",
        "learning_rate = 0.008\n",
        "keep_probability = 0.75"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXtE1-pMwYYd",
        "outputId": "d33c106d-e62c-4451-a4cd-34357f21aa4d"
      },
      "source": [
        "# Build the graph\n",
        "train_graph = tf.Graph()\n",
        "# Set the graph to default to ensure that it is ready for training\n",
        "with train_graph.as_default():\n",
        "    \n",
        "    # Load the model inputs    \n",
        "    input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length = model_inputs()\n",
        "\n",
        "    # Create the training and inference logits\n",
        "    training_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
        "                                                      targets, \n",
        "                                                      keep_prob,   \n",
        "                                                      text_length,\n",
        "                                                      summary_length,\n",
        "                                                      max_summary_length,\n",
        "                                                      len(vocab_to_int)+1,\n",
        "                                                      rnn_size, \n",
        "                                                      num_layers, \n",
        "                                                      vocab_to_int,\n",
        "                                                      batch_size)\n",
        "    \n",
        "    # Create tensors for the training logits and inference logits\n",
        "    training_logits = tf.identity(training_logits.rnn_output, 'logits')\n",
        "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
        "    \n",
        "    # Create the weights for sequence_loss\n",
        "    masks = tf.sequence_mask(summary_length, max_summary_length, dtype=tf.float32, name='masks')\n",
        "\n",
        "    with tf.name_scope(\"optimization\"):\n",
        "        # Loss function\n",
        "        cost = tf.contrib.seq2seq.sequence_loss(\n",
        "            training_logits,\n",
        "            targets,\n",
        "            masks)\n",
        "\n",
        "        # Optimizer\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "\n",
        "        # Gradient Clipping\n",
        "        gradients = optimizer.compute_gradients(cost)\n",
        "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
        "        train_op = optimizer.apply_gradients(capped_gradients)\n",
        "print(\"Graph is built.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-25-9390cb26606a>:7: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-25-9390cb26606a>:20: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:958: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:962: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Graph is built.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mV2_jNFnT-kr"
      },
      "source": [
        "###Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oym97vGxxxrg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48be514e-b74e-4cdd-b4ef-156d3a92631d"
      },
      "source": [
        "print(\"The shortest text length:\", len(sorted_texts[0]))\n",
        "print(\"The longest text length:\",len(sorted_texts[-1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The shortest text length: 41\n",
            "The longest text length: 333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKkK00w6UGnr"
      },
      "source": [
        "learning_rate_decay = 0.95\n",
        "min_learning_rate = 5e-4\n",
        "display_step = 20\n",
        "stop_early = 0\n",
        "stop = 6 # If the update loss does not decrease in 3 consecutive update checks, stop training\n",
        "per_epoch = 3\n",
        "update_check = (len(sorted_texts)//batch_size//per_epoch)-1\n",
        "\n",
        "update_loss = 0 \n",
        "batch_loss = 0\n",
        "summary_update_loss = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aANZ2rBkVPBv",
        "outputId": "99325558-7615-4e8f-fe6d-3e8ef7ceb66e"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "checkpoint = \"/content/drive/MyDrive/Checkpoints\"  #100k sentence\"  #300k sentence\n",
        "with tf.Session(graph=train_graph) as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    # If we want to continue training a previous session\n",
        "    # loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
        "    # loader.restore(sess, checkpoint)\n",
        "    #sess.run(tf.local_variables_initializer())\n",
        "\n",
        "    for epoch_i in range(1, epochs+1):\n",
        "        update_loss = 0\n",
        "        batch_loss = 0\n",
        "        for batch_i, (summaries_batch, texts_batch, summaries_lengths, texts_lengths) in enumerate(\n",
        "                get_batches(sorted_summaries, sorted_texts, batch_size)):\n",
        "            start_time = time.time()\n",
        "            _, loss = sess.run(\n",
        "                [train_op, cost],\n",
        "                {input_data: texts_batch,\n",
        "                 targets: summaries_batch,\n",
        "                 lr: learning_rate,\n",
        "                 summary_length: summaries_lengths,\n",
        "                 text_length: texts_lengths,\n",
        "                 keep_prob: keep_probability})\n",
        "\n",
        "            batch_loss += loss\n",
        "            update_loss += loss\n",
        "            end_time = time.time()\n",
        "            batch_time = end_time - start_time\n",
        "\n",
        "            if batch_i % display_step == 0 and batch_i > 0:\n",
        "                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
        "                      .format(epoch_i,\n",
        "                              epochs, \n",
        "                              batch_i, \n",
        "                              len(sorted_texts) // batch_size, \n",
        "                              batch_loss / display_step, \n",
        "                              batch_time*display_step))\n",
        "                batch_loss = 0\n",
        "                \n",
        "                #saver = tf.train.Saver() \n",
        "                #saver.save(sess, checkpoint)\n",
        "                \n",
        "            if batch_i % update_check == 0 and batch_i > 0:\n",
        "                print(\"Average loss for this update:\", round(update_loss/update_check,3))\n",
        "                summary_update_loss.append(update_loss)\n",
        "                \n",
        "              \n",
        "                  \n",
        "                # If the update loss is at a new minimum, save the model\n",
        "                if update_loss <= min(summary_update_loss):\n",
        "                    print('New Record!') \n",
        "                    stop_early = 0\n",
        "                    saver = tf.train.Saver() \n",
        "                    saver.save(sess, checkpoint)\n",
        "\n",
        "                else:\n",
        "                    print(\"No Improvement.\")\n",
        "                    stop_early += 1\n",
        "                    if stop_early == stop:\n",
        "                        break\n",
        "                update_loss = 0\n",
        "            \n",
        "                    \n",
        "        # Reduce learning rate, but not below its minimum value\n",
        "        learning_rate *= learning_rate_decay\n",
        "        if learning_rate < min_learning_rate:\n",
        "            learning_rate = min_learning_rate\n",
        "        \n",
        "        if stop_early == stop:\n",
        "            print(\"Stopping Training.\")\n",
        "            break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average loss for this update: 8.788\n",
            "New Record!\n",
            "Epoch   1/100 Batch   20/59 - Loss:  8.631, Seconds: 13.26\n",
            "Average loss for this update: 6.997\n",
            "New Record!\n",
            "Epoch   1/100 Batch   40/59 - Loss:  6.981, Seconds: 15.22\n",
            "Average loss for this update: 6.781\n",
            "New Record!\n",
            "Average loss for this update: 6.441\n",
            "New Record!\n",
            "Epoch   2/100 Batch   20/59 - Loss:  6.388, Seconds: 13.64\n",
            "Average loss for this update: 5.751\n",
            "New Record!\n",
            "Epoch   2/100 Batch   40/59 - Loss:  5.746, Seconds: 15.53\n",
            "Average loss for this update: 5.64\n",
            "New Record!\n",
            "Average loss for this update: 5.606\n",
            "New Record!\n",
            "Epoch   3/100 Batch   20/59 - Loss:  5.562, Seconds: 13.79\n",
            "Average loss for this update: 5.046\n",
            "New Record!\n",
            "Epoch   3/100 Batch   40/59 - Loss:  5.045, Seconds: 15.48\n",
            "Average loss for this update: 4.991\n",
            "New Record!\n",
            "Average loss for this update: 5.02\n",
            "No Improvement.\n",
            "Epoch   4/100 Batch   20/59 - Loss:  4.987, Seconds: 13.70\n",
            "Average loss for this update: 4.574\n",
            "New Record!\n",
            "Epoch   4/100 Batch   40/59 - Loss:  4.562, Seconds: 15.37\n",
            "Average loss for this update: 4.535\n",
            "New Record!\n",
            "Average loss for this update: 4.602\n",
            "No Improvement.\n",
            "Epoch   5/100 Batch   20/59 - Loss:  4.571, Seconds: 13.35\n",
            "Average loss for this update: 4.201\n",
            "New Record!\n",
            "Epoch   5/100 Batch   40/59 - Loss:  4.198, Seconds: 15.30\n",
            "Average loss for this update: 4.18\n",
            "New Record!\n",
            "Average loss for this update: 4.265\n",
            "No Improvement.\n",
            "Epoch   6/100 Batch   20/59 - Loss:  4.232, Seconds: 13.42\n",
            "Average loss for this update: 3.906\n",
            "New Record!\n",
            "Epoch   6/100 Batch   40/59 - Loss:  3.904, Seconds: 15.48\n",
            "Average loss for this update: 3.892\n",
            "New Record!\n",
            "Average loss for this update: 3.956\n",
            "No Improvement.\n",
            "Epoch   7/100 Batch   20/59 - Loss:  3.924, Seconds: 13.27\n",
            "Average loss for this update: 3.625\n",
            "New Record!\n",
            "Epoch   7/100 Batch   40/59 - Loss:  3.626, Seconds: 15.41\n",
            "Average loss for this update: 3.611\n",
            "New Record!\n",
            "Average loss for this update: 3.699\n",
            "No Improvement.\n",
            "Epoch   8/100 Batch   20/59 - Loss:  3.665, Seconds: 13.30\n",
            "Average loss for this update: 3.344\n",
            "New Record!\n",
            "Epoch   8/100 Batch   40/59 - Loss:  3.343, Seconds: 15.28\n",
            "Average loss for this update: 3.32\n",
            "New Record!\n",
            "Average loss for this update: 3.419\n",
            "No Improvement.\n",
            "Epoch   9/100 Batch   20/59 - Loss:  3.388, Seconds: 13.64\n",
            "Average loss for this update: 3.073\n",
            "New Record!\n",
            "Epoch   9/100 Batch   40/59 - Loss:  3.068, Seconds: 15.38\n",
            "Average loss for this update: 3.065\n",
            "New Record!\n",
            "Average loss for this update: 3.155\n",
            "No Improvement.\n",
            "Epoch  10/100 Batch   20/59 - Loss:  3.121, Seconds: 13.24\n",
            "Average loss for this update: 2.786\n",
            "New Record!\n",
            "Epoch  10/100 Batch   40/59 - Loss:  2.781, Seconds: 15.26\n",
            "Average loss for this update: 2.809\n",
            "No Improvement.\n",
            "Average loss for this update: 2.912\n",
            "No Improvement.\n",
            "Epoch  11/100 Batch   20/59 - Loss:  2.881, Seconds: 13.41\n",
            "Average loss for this update: 2.548\n",
            "New Record!\n",
            "Epoch  11/100 Batch   40/59 - Loss:  2.544, Seconds: 15.59\n",
            "Average loss for this update: 2.579\n",
            "No Improvement.\n",
            "Average loss for this update: 2.638\n",
            "No Improvement.\n",
            "Epoch  12/100 Batch   20/59 - Loss:  2.604, Seconds: 13.49\n",
            "Average loss for this update: 2.306\n",
            "New Record!\n",
            "Epoch  12/100 Batch   40/59 - Loss:  2.305, Seconds: 15.48\n",
            "Average loss for this update: 2.324\n",
            "No Improvement.\n",
            "Average loss for this update: 2.364\n",
            "No Improvement.\n",
            "Epoch  13/100 Batch   20/59 - Loss:  2.336, Seconds: 13.47\n",
            "Average loss for this update: 2.123\n",
            "New Record!\n",
            "Epoch  13/100 Batch   40/59 - Loss:  2.127, Seconds: 15.37\n",
            "Average loss for this update: 2.177\n",
            "No Improvement.\n",
            "Average loss for this update: 2.186\n",
            "No Improvement.\n",
            "Epoch  14/100 Batch   20/59 - Loss:  2.160, Seconds: 13.35\n",
            "Average loss for this update: 1.913\n",
            "New Record!\n",
            "Epoch  14/100 Batch   40/59 - Loss:  1.918, Seconds: 15.42\n",
            "Average loss for this update: 1.978\n",
            "No Improvement.\n",
            "Average loss for this update: 2.031\n",
            "No Improvement.\n",
            "Epoch  15/100 Batch   20/59 - Loss:  2.000, Seconds: 13.58\n",
            "Average loss for this update: 1.77\n",
            "New Record!\n",
            "Epoch  15/100 Batch   40/59 - Loss:  1.767, Seconds: 15.54\n",
            "Average loss for this update: 1.796\n",
            "No Improvement.\n",
            "Average loss for this update: 1.913\n",
            "No Improvement.\n",
            "Epoch  16/100 Batch   20/59 - Loss:  1.885, Seconds: 13.51\n",
            "Average loss for this update: 1.645\n",
            "New Record!\n",
            "Epoch  16/100 Batch   40/59 - Loss:  1.663, Seconds: 15.62\n",
            "Average loss for this update: 1.72\n",
            "No Improvement.\n",
            "Average loss for this update: 1.79\n",
            "No Improvement.\n",
            "Epoch  17/100 Batch   20/59 - Loss:  1.762, Seconds: 13.39\n",
            "Average loss for this update: 1.542\n",
            "New Record!\n",
            "Epoch  17/100 Batch   40/59 - Loss:  1.551, Seconds: 15.36\n",
            "Average loss for this update: 1.611\n",
            "No Improvement.\n",
            "Average loss for this update: 1.708\n",
            "No Improvement.\n",
            "Epoch  18/100 Batch   20/59 - Loss:  1.693, Seconds: 13.38\n",
            "Average loss for this update: 1.534\n",
            "New Record!\n",
            "Epoch  18/100 Batch   40/59 - Loss:  1.527, Seconds: 15.37\n",
            "Average loss for this update: 1.555\n",
            "No Improvement.\n",
            "Average loss for this update: 1.685\n",
            "No Improvement.\n",
            "Epoch  19/100 Batch   20/59 - Loss:  1.660, Seconds: 13.42\n",
            "Average loss for this update: 1.496\n",
            "New Record!\n",
            "Epoch  19/100 Batch   40/59 - Loss:  1.496, Seconds: 15.58\n",
            "Average loss for this update: 1.486\n",
            "New Record!\n",
            "Average loss for this update: 1.605\n",
            "No Improvement.\n",
            "Epoch  20/100 Batch   20/59 - Loss:  1.584, Seconds: 13.36\n",
            "Average loss for this update: 1.392\n",
            "New Record!\n",
            "Epoch  20/100 Batch   40/59 - Loss:  1.394, Seconds: 15.57\n",
            "Average loss for this update: 1.434\n",
            "No Improvement.\n",
            "Average loss for this update: 1.524\n",
            "No Improvement.\n",
            "Epoch  21/100 Batch   20/59 - Loss:  1.506, Seconds: 13.40\n",
            "Average loss for this update: 1.339\n",
            "New Record!\n",
            "Epoch  21/100 Batch   40/59 - Loss:  1.334, Seconds: 15.48\n",
            "Average loss for this update: 1.358\n",
            "No Improvement.\n",
            "Average loss for this update: 1.477\n",
            "No Improvement.\n",
            "Epoch  22/100 Batch   20/59 - Loss:  1.458, Seconds: 13.44\n",
            "Average loss for this update: 1.294\n",
            "New Record!\n",
            "Epoch  22/100 Batch   40/59 - Loss:  1.301, Seconds: 15.41\n",
            "Average loss for this update: 1.348\n",
            "No Improvement.\n",
            "Average loss for this update: 1.491\n",
            "No Improvement.\n",
            "Epoch  23/100 Batch   20/59 - Loss:  1.477, Seconds: 13.58\n",
            "Average loss for this update: 1.323\n",
            "No Improvement.\n",
            "Epoch  23/100 Batch   40/59 - Loss:  1.311, Seconds: 15.35\n",
            "Average loss for this update: 1.298\n",
            "No Improvement.\n",
            "Average loss for this update: 1.374\n",
            "No Improvement.\n",
            "Epoch  24/100 Batch   20/59 - Loss:  1.352, Seconds: 13.35\n",
            "Average loss for this update: 1.228\n",
            "New Record!\n",
            "Epoch  24/100 Batch   40/59 - Loss:  1.236, Seconds: 15.38\n",
            "Average loss for this update: 1.255\n",
            "No Improvement.\n",
            "Average loss for this update: 1.316\n",
            "No Improvement.\n",
            "Epoch  25/100 Batch   20/59 - Loss:  1.290, Seconds: 13.26\n",
            "Average loss for this update: 1.156\n",
            "New Record!\n",
            "Epoch  25/100 Batch   40/59 - Loss:  1.167, Seconds: 15.59\n",
            "Average loss for this update: 1.199\n",
            "No Improvement.\n",
            "Average loss for this update: 1.29\n",
            "No Improvement.\n",
            "Epoch  26/100 Batch   20/59 - Loss:  1.270, Seconds: 13.51\n",
            "Average loss for this update: 1.135\n",
            "New Record!\n",
            "Epoch  26/100 Batch   40/59 - Loss:  1.142, Seconds: 15.16\n",
            "Average loss for this update: 1.188\n",
            "No Improvement.\n",
            "Average loss for this update: 1.302\n",
            "No Improvement.\n",
            "Epoch  27/100 Batch   20/59 - Loss:  1.279, Seconds: 13.39\n",
            "Average loss for this update: 1.105\n",
            "New Record!\n",
            "Epoch  27/100 Batch   40/59 - Loss:  1.111, Seconds: 15.10\n",
            "Average loss for this update: 1.152\n",
            "No Improvement.\n",
            "Average loss for this update: 1.29\n",
            "No Improvement.\n",
            "Epoch  28/100 Batch   20/59 - Loss:  1.266, Seconds: 13.40\n",
            "Average loss for this update: 1.105\n",
            "New Record!\n",
            "Epoch  28/100 Batch   40/59 - Loss:  1.109, Seconds: 15.21\n",
            "Average loss for this update: 1.131\n",
            "No Improvement.\n",
            "Average loss for this update: 1.241\n",
            "No Improvement.\n",
            "Epoch  29/100 Batch   20/59 - Loss:  1.223, Seconds: 13.55\n",
            "Average loss for this update: 1.091\n",
            "New Record!\n",
            "Epoch  29/100 Batch   40/59 - Loss:  1.097, Seconds: 15.27\n",
            "Average loss for this update: 1.131\n",
            "No Improvement.\n",
            "Average loss for this update: 1.226\n",
            "No Improvement.\n",
            "Epoch  30/100 Batch   20/59 - Loss:  1.214, Seconds: 13.32\n",
            "Average loss for this update: 1.083\n",
            "New Record!\n",
            "Epoch  30/100 Batch   40/59 - Loss:  1.082, Seconds: 15.56\n",
            "Average loss for this update: 1.133\n",
            "No Improvement.\n",
            "Average loss for this update: 1.241\n",
            "No Improvement.\n",
            "Epoch  31/100 Batch   20/59 - Loss:  1.223, Seconds: 13.43\n",
            "Average loss for this update: 1.082\n",
            "New Record!\n",
            "Epoch  31/100 Batch   40/59 - Loss:  1.081, Seconds: 15.51\n",
            "Average loss for this update: 1.116\n",
            "No Improvement.\n",
            "Average loss for this update: 1.264\n",
            "No Improvement.\n",
            "Epoch  32/100 Batch   20/59 - Loss:  1.243, Seconds: 13.30\n",
            "Average loss for this update: 1.084\n",
            "No Improvement.\n",
            "Epoch  32/100 Batch   40/59 - Loss:  1.091, Seconds: 15.24\n",
            "Average loss for this update: 1.123\n",
            "No Improvement.\n",
            "Average loss for this update: 1.217\n",
            "No Improvement.\n",
            "Epoch  33/100 Batch   20/59 - Loss:  1.203, Seconds: 13.32\n",
            "Average loss for this update: 1.103\n",
            "No Improvement.\n",
            "Stopping Training.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQDOoGlkWKTq"
      },
      "source": [
        "from google.colab import output\n",
        "output.eval_js('new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgPncRkts5LA",
        "outputId": "384491e0-39fa-4de9-b7e0-d2de43b52019"
      },
      "source": [
        "checkpoint = \"/content/drive/MyDrive/Checkpoints/Checkpoints\" \n",
        "\n",
        "loaded_graph = tf.Graph()\n",
        "with tf.Session(graph=loaded_graph) as sess:\n",
        "    # Load saved model\n",
        "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
        "    loader.restore(sess, checkpoint)\n",
        "    names = []\n",
        "    [names.append(n.name) for n in loaded_graph.as_graph_def().node]\n",
        "names"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/drive/MyDrive/Checkpoints/Checkpoints\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['input',\n",
              " 'targets',\n",
              " 'learning_rate',\n",
              " 'keep_prob',\n",
              " 'summary_length',\n",
              " 'Const',\n",
              " 'max_dec_len',\n",
              " 'text_length',\n",
              " 'ReverseV2/axis',\n",
              " 'ReverseV2',\n",
              " 'embedding_lookup/params_0',\n",
              " 'embedding_lookup/axis',\n",
              " 'embedding_lookup',\n",
              " 'embedding_lookup/Identity',\n",
              " 'encoder_0/DropoutWrapperInit/Const',\n",
              " 'encoder_0/DropoutWrapperInit/Const_1',\n",
              " 'encoder_0/DropoutWrapperInit_1/Const',\n",
              " 'encoder_0/DropoutWrapperInit_1/Const_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Rank',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/range/start',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/range/delta',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/range',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/concat/values_0',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/concat/axis',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/concat',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/transpose',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/sequence_length',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Shape',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice/stack',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice/stack_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice/stack_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims/dim',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/Const',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/concat/axis',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/concat',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/zeros/Const',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/zeros',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_1/dim',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/Const_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_2/dim',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/Const_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/concat_1/axis',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/concat_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/zeros_1/Const',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/zeros_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_3/dim',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/Const_3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Shape_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/stack',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Equal',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Const',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/All',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Assert/Const',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Assert/Const_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Assert/Assert/data_0',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Assert/Assert/data_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Assert/Assert',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/CheckSeqLen',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Shape_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice_1/stack',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice_1/stack_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice_1/stack_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Shape_3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice_2/stack',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice_2/stack_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice_2/stack_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/strided_slice_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/ExpandDims/dim',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/ExpandDims',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Const_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/concat_1/axis',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/concat_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/zeros/Const',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/zeros',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Const_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Min',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Const_3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Max',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/time',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArray',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArray_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayUnstack/Shape',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayUnstack/strided_slice/stack',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayUnstack/strided_slice/stack_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayUnstack/strided_slice/stack_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayUnstack/strided_slice',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayUnstack/range/start',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayUnstack/range/delta',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayUnstack/range',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Maximum/x',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Maximum',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Minimum',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/iteration_counter',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Enter',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Enter_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Enter_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Enter_3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Enter_4',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Merge',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Merge_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Merge_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Merge_3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Merge_4',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Less/Enter',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Less',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Less_1/Enter',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Less_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/LogicalAnd',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/LoopCond',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Switch',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Switch_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Switch_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Switch_3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Switch_4',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Identity',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Identity_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Identity_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Identity_3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Identity_4',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/add/y',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/add',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/TensorArrayReadV3/Enter',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/TensorArrayReadV3/Enter_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/TensorArrayReadV3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/GreaterEqual/Enter',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/GreaterEqual',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/sub/x',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/sub/Enter',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/sub',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/Shape',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/random_uniform/min',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/random_uniform/max',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/random_uniform/RandomUniform',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/random_uniform/sub',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/random_uniform/mul',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/random_uniform',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/sub/x',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/sub',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/truediv/x',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/truediv',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/GreaterEqual',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/mul',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/Cast',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/dropout/mul_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/shape',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/min',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/max',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/RandomUniform',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/sub',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/mul',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/kernel',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/kernel/Assign',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/kernel/read',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/bias/Initializer/zeros/shape_as_tensor',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/bias/Initializer/zeros/Const',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/bias/Initializer/zeros',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/bias',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/bias/Assign',\n",
              " 'encoder_0/bidirectional_rnn/fw/lstm_cell/bias/read',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/concat/axis',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/concat',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/MatMul/Enter',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/MatMul',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/BiasAdd/Enter',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/BiasAdd',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/Const',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/split/split_dim',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/split',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/add/y',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/add',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/Sigmoid',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/mul',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/Sigmoid_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/Tanh',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/mul_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/add_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/Sigmoid_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/Tanh_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/lstm_cell/mul_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Select/Enter',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Select',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Select_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Select_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/TensorArrayWrite/TensorArrayWriteV3/Enter',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/TensorArrayWrite/TensorArrayWriteV3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/add_1/y',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/add_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/NextIteration',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/NextIteration_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/NextIteration_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/NextIteration_3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/NextIteration_4',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Exit',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Exit_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Exit_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Exit_3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/while/Exit_4',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayStack/TensorArraySizeV3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayStack/range/start',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayStack/range/delta',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayStack/range',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/TensorArrayStack/TensorArrayGatherV3',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Const_4',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/Rank_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/range_1/start',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/range_1/delta',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/range_1',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/concat_2/values_0',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/concat_2/axis',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/concat_2',\n",
              " 'encoder_0/bidirectional_rnn/fw/fw/transpose_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/ReverseSequence',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Rank',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/range/start',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/range/delta',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/range',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/concat/values_0',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/concat/axis',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/concat',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/transpose',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/sequence_length',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Shape',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice/stack',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice/stack_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice/stack_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims/dim',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/Const',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/concat/axis',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/concat',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/zeros/Const',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/zeros',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_1/dim',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/Const_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_2/dim',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/Const_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/concat_1/axis',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/concat_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/zeros_1/Const',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/zeros_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_3/dim',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/Const_3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Shape_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/stack',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Equal',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Const',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/All',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Assert/Const',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Assert/Const_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Assert/Assert/data_0',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Assert/Assert/data_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Assert/Assert',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/CheckSeqLen',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Shape_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice_1/stack',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice_1/stack_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice_1/stack_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Shape_3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice_2/stack',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice_2/stack_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice_2/stack_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/strided_slice_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/ExpandDims/dim',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/ExpandDims',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Const_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/concat_1/axis',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/concat_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/zeros/Const',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/zeros',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Const_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Min',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Const_3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Max',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/time',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArray',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArray_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayUnstack/Shape',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayUnstack/strided_slice/stack',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayUnstack/strided_slice/stack_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayUnstack/strided_slice/stack_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayUnstack/strided_slice',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayUnstack/range/start',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayUnstack/range/delta',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayUnstack/range',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Maximum/x',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Maximum',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Minimum',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/iteration_counter',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Enter',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Enter_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Enter_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Enter_3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Enter_4',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Merge',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Merge_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Merge_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Merge_3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Merge_4',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Less/Enter',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Less',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Less_1/Enter',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Less_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/LogicalAnd',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/LoopCond',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Switch',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Switch_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Switch_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Switch_3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Switch_4',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Identity',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Identity_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Identity_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Identity_3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Identity_4',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/add/y',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/add',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/TensorArrayReadV3/Enter',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/TensorArrayReadV3/Enter_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/TensorArrayReadV3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/GreaterEqual/Enter',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/GreaterEqual',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/sub/x',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/sub/Enter',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/sub',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/Shape',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/random_uniform/min',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/random_uniform/max',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/random_uniform/RandomUniform',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/random_uniform/sub',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/random_uniform/mul',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/random_uniform',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/sub/x',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/sub',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/truediv/x',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/truediv',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/GreaterEqual',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/mul',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/Cast',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/dropout/mul_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/shape',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/min',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/max',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/RandomUniform',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/sub',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/mul',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/kernel',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/kernel/Assign',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/kernel/read',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/bias/Initializer/zeros/shape_as_tensor',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/bias/Initializer/zeros/Const',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/bias/Initializer/zeros',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/bias',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/bias/Assign',\n",
              " 'encoder_0/bidirectional_rnn/bw/lstm_cell/bias/read',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/concat/axis',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/concat',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/MatMul/Enter',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/MatMul',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/BiasAdd/Enter',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/BiasAdd',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/Const',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/split/split_dim',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/split',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/add/y',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/add',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/Sigmoid',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/mul',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/Sigmoid_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/Tanh',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/mul_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/add_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/Sigmoid_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/Tanh_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/lstm_cell/mul_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Select/Enter',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Select',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Select_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Select_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/TensorArrayWrite/TensorArrayWriteV3/Enter',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/TensorArrayWrite/TensorArrayWriteV3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/add_1/y',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/add_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/NextIteration',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/NextIteration_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/NextIteration_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/NextIteration_3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/NextIteration_4',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Exit',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Exit_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Exit_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Exit_3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/while/Exit_4',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayStack/TensorArraySizeV3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayStack/range/start',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayStack/range/delta',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayStack/range',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/TensorArrayStack/TensorArrayGatherV3',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Const_4',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/Rank_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/range_1/start',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/range_1/delta',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/range_1',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/concat_2/values_0',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/concat_2/axis',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/concat_2',\n",
              " 'encoder_0/bidirectional_rnn/bw/bw/transpose_1',\n",
              " 'encoder_0/ReverseSequence',\n",
              " 'encoder_1/DropoutWrapperInit/Const',\n",
              " 'encoder_1/DropoutWrapperInit/Const_1',\n",
              " 'encoder_1/DropoutWrapperInit_1/Const',\n",
              " 'encoder_1/DropoutWrapperInit_1/Const_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Rank',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/range/start',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/range/delta',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/range',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/concat/values_0',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/concat/axis',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/concat',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/transpose',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/sequence_length',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Shape',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice/stack',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice/stack_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice/stack_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims/dim',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/Const',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/concat/axis',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/concat',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/zeros/Const',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/zeros',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_1/dim',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/Const_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_2/dim',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/Const_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/concat_1/axis',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/concat_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/zeros_1/Const',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/zeros_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_3/dim',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/Const_3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Shape_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/stack',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Equal',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Const',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/All',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Assert/Const',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Assert/Const_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Assert/Assert/data_0',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Assert/Assert/data_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Assert/Assert',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/CheckSeqLen',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Shape_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice_1/stack',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice_1/stack_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice_1/stack_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Shape_3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice_2/stack',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice_2/stack_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice_2/stack_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/strided_slice_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/ExpandDims/dim',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/ExpandDims',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Const_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/concat_1/axis',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/concat_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/zeros/Const',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/zeros',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Const_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Min',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Const_3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Max',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/time',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArray',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArray_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayUnstack/Shape',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayUnstack/strided_slice/stack',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayUnstack/strided_slice/stack_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayUnstack/strided_slice/stack_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayUnstack/strided_slice',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayUnstack/range/start',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayUnstack/range/delta',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayUnstack/range',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Maximum/x',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Maximum',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Minimum',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/iteration_counter',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Enter',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Enter_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Enter_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Enter_3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Enter_4',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Merge',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Merge_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Merge_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Merge_3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Merge_4',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Less/Enter',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Less',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Less_1/Enter',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Less_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/LogicalAnd',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/LoopCond',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Switch',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Switch_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Switch_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Switch_3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Switch_4',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Identity',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Identity_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Identity_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Identity_3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Identity_4',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/add/y',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/add',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/TensorArrayReadV3/Enter',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/TensorArrayReadV3/Enter_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/TensorArrayReadV3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/GreaterEqual/Enter',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/GreaterEqual',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/sub/x',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/sub/Enter',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/sub',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/Shape',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/random_uniform/min',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/random_uniform/max',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/random_uniform/RandomUniform',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/random_uniform/sub',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/random_uniform/mul',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/random_uniform',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/sub/x',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/sub',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/truediv/x',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/truediv',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/GreaterEqual',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/mul',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/Cast',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/dropout/mul_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/shape',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/min',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/max',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/RandomUniform',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/sub',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/mul',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/kernel',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/kernel/Assign',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/kernel/read',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/bias/Initializer/zeros/shape_as_tensor',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/bias/Initializer/zeros/Const',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/bias/Initializer/zeros',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/bias',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/bias/Assign',\n",
              " 'encoder_1/bidirectional_rnn/fw/lstm_cell/bias/read',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/concat/axis',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/concat',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/MatMul/Enter',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/MatMul',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/BiasAdd/Enter',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/BiasAdd',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/Const',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/split/split_dim',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/split',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/add/y',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/add',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/Sigmoid',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/mul',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/Sigmoid_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/Tanh',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/mul_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/add_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/Sigmoid_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/Tanh_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/lstm_cell/mul_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Select/Enter',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Select',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Select_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Select_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/TensorArrayWrite/TensorArrayWriteV3/Enter',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/TensorArrayWrite/TensorArrayWriteV3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/add_1/y',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/add_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/NextIteration',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/NextIteration_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/NextIteration_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/NextIteration_3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/NextIteration_4',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/while/Exit_4',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayStack/TensorArraySizeV3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayStack/range/start',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayStack/range/delta',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayStack/range',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/TensorArrayStack/TensorArrayGatherV3',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Const_4',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/Rank_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/range_1/start',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/range_1/delta',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/range_1',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/concat_2/values_0',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/concat_2/axis',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/concat_2',\n",
              " 'encoder_1/bidirectional_rnn/fw/fw/transpose_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/ReverseSequence',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Rank',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/range/start',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/range/delta',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/range',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/concat/values_0',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/concat/axis',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/concat',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/transpose',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/sequence_length',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Shape',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice/stack',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice/stack_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice/stack_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims/dim',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/Const',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/concat/axis',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/concat',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/zeros/Const',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/zeros',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_1/dim',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/Const_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_2/dim',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/Const_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/concat_1/axis',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/concat_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/zeros_1/Const',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/zeros_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_3/dim',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/DropoutWrapperZeroState/LSTMCellZeroState/Const_3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Shape_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/stack',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Equal',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Const',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/All',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Assert/Const',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Assert/Const_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Assert/Assert/data_0',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Assert/Assert/data_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Assert/Assert',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/CheckSeqLen',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Shape_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice_1/stack',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice_1/stack_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice_1/stack_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Shape_3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice_2/stack',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice_2/stack_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice_2/stack_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/strided_slice_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/ExpandDims/dim',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/ExpandDims',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Const_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/concat_1/axis',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/concat_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/zeros/Const',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/zeros',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Const_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Min',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Const_3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Max',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/time',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArray',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArray_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayUnstack/Shape',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayUnstack/strided_slice/stack',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayUnstack/strided_slice/stack_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayUnstack/strided_slice/stack_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayUnstack/strided_slice',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayUnstack/range/start',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayUnstack/range/delta',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayUnstack/range',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Maximum/x',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Maximum',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Minimum',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/iteration_counter',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Enter',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Enter_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Enter_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Enter_3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Enter_4',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Merge',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Merge_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Merge_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Merge_3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Merge_4',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Less/Enter',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Less',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Less_1/Enter',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Less_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/LogicalAnd',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/LoopCond',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Switch',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Switch_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Switch_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Switch_3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Switch_4',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Identity',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Identity_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Identity_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Identity_3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Identity_4',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/add/y',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/add',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/TensorArrayReadV3/Enter',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/TensorArrayReadV3/Enter_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/TensorArrayReadV3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/GreaterEqual/Enter',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/GreaterEqual',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/sub/x',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/sub/Enter',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/sub',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/Shape',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/random_uniform/min',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/random_uniform/max',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/random_uniform/RandomUniform',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/random_uniform/sub',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/random_uniform/mul',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/random_uniform',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/sub/x',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/sub',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/truediv/x',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/truediv',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/GreaterEqual',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/mul',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/Cast',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/dropout/mul_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/shape',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/min',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/max',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/RandomUniform',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/sub',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform/mul',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/kernel/Initializer/random_uniform',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/kernel',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/kernel/Assign',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/kernel/read',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/bias/Initializer/zeros/shape_as_tensor',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/bias/Initializer/zeros/Const',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/bias/Initializer/zeros',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/bias',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/bias/Assign',\n",
              " 'encoder_1/bidirectional_rnn/bw/lstm_cell/bias/read',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/concat/axis',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/concat',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/MatMul/Enter',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/MatMul',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/BiasAdd/Enter',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/BiasAdd',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/Const',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/split/split_dim',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/split',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/add/y',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/add',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/Sigmoid',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/mul',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/Sigmoid_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/Tanh',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/mul_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/add_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/Sigmoid_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/Tanh_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/lstm_cell/mul_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Select/Enter',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Select',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Select_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Select_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/TensorArrayWrite/TensorArrayWriteV3/Enter',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/TensorArrayWrite/TensorArrayWriteV3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/add_1/y',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/add_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/NextIteration',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/NextIteration_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/NextIteration_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/NextIteration_3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/NextIteration_4',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Exit',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Exit_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Exit_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Exit_3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/while/Exit_4',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayStack/TensorArraySizeV3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayStack/range/start',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayStack/range/delta',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayStack/range',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/TensorArrayStack/TensorArrayGatherV3',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Const_4',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/Rank_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/range_1/start',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/range_1/delta',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/range_1',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/concat_2/values_0',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/concat_2/axis',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/concat_2',\n",
              " 'encoder_1/bidirectional_rnn/bw/bw/transpose_1',\n",
              " 'encoder_1/ReverseSequence',\n",
              " 'encoder_2/DropoutWrapperInit/Const',\n",
              " 'encoder_2/DropoutWrapperInit/Const_1',\n",
              " 'encoder_2/DropoutWrapperInit_1/Const',\n",
              " 'encoder_2/DropoutWrapperInit_1/Const_1',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/Rank',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/range/start',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/range/delta',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/range',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/concat/values_0',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/concat/axis',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/concat',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/transpose',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/sequence_length',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/Shape',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/strided_slice/stack',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/strided_slice/stack_1',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/strided_slice/stack_2',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/strided_slice',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims/dim',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/Const',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/concat/axis',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/concat',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/zeros/Const',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/zeros',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_1/dim',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_1',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/Const_1',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_2/dim',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_2',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/Const_2',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/concat_1/axis',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/concat_1',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/zeros_1/Const',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/zeros_1',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_3/dim',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/ExpandDims_3',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/DropoutWrapperZeroState/LSTMCellZeroState/Const_3',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/Shape_1',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/stack',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/Equal',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/Const',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/All',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/Assert/Const',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/Assert/Const_1',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/Assert/Assert/data_0',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/Assert/Assert/data_2',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/Assert/Assert',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/CheckSeqLen',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/Shape_2',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/strided_slice_1/stack',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/strided_slice_1/stack_1',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/strided_slice_1/stack_2',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/strided_slice_1',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/Shape_3',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/strided_slice_2/stack',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/strided_slice_2/stack_1',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/strided_slice_2/stack_2',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/strided_slice_2',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/ExpandDims/dim',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/ExpandDims',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/Const_1',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/concat_1/axis',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/concat_1',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/zeros/Const',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/zeros',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/Const_2',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/Min',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/Const_3',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/Max',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/time',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/TensorArray',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/TensorArray_1',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/TensorArrayUnstack/Shape',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/TensorArrayUnstack/strided_slice/stack',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/TensorArrayUnstack/strided_slice/stack_1',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/TensorArrayUnstack/strided_slice/stack_2',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/TensorArrayUnstack/strided_slice',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/TensorArrayUnstack/range/start',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/TensorArrayUnstack/range/delta',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/TensorArrayUnstack/range',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/Maximum/x',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/Maximum',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/Minimum',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/iteration_counter',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/Enter',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/Enter_1',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/Enter_2',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/Enter_3',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/Enter_4',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/Merge',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/Merge_1',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/Merge_2',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/Merge_3',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/Merge_4',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/Less/Enter',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/Less',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/Less_1/Enter',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/Less_1',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/LogicalAnd',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/LoopCond',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/Switch',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/Switch_1',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/Switch_2',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/Switch_3',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/Switch_4',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/Identity',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/Identity_1',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/Identity_2',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/Identity_3',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/Identity_4',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/add/y',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/add',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/TensorArrayReadV3/Enter',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/TensorArrayReadV3/Enter_1',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/TensorArrayReadV3',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/GreaterEqual/Enter',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/GreaterEqual',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/sub/x',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/sub/Enter',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/sub',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/dropout/Shape',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/dropout/random_uniform/min',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/dropout/random_uniform/max',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/dropout/random_uniform/RandomUniform',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/dropout/random_uniform/sub',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/dropout/random_uniform/mul',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/dropout/random_uniform',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/dropout/sub/x',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/dropout/sub',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/dropout/truediv/x',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/dropout/truediv',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/dropout/GreaterEqual',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/dropout/mul',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/dropout/Cast',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/dropout/mul_1',\n",
              " 'encoder_2/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/shape',\n",
              " 'encoder_2/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/min',\n",
              " 'encoder_2/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/max',\n",
              " 'encoder_2/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/RandomUniform',\n",
              " 'encoder_2/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/sub',\n",
              " 'encoder_2/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform/mul',\n",
              " 'encoder_2/bidirectional_rnn/fw/lstm_cell/kernel/Initializer/random_uniform',\n",
              " 'encoder_2/bidirectional_rnn/fw/lstm_cell/kernel',\n",
              " 'encoder_2/bidirectional_rnn/fw/lstm_cell/kernel/Assign',\n",
              " 'encoder_2/bidirectional_rnn/fw/lstm_cell/kernel/read',\n",
              " 'encoder_2/bidirectional_rnn/fw/lstm_cell/bias/Initializer/zeros/shape_as_tensor',\n",
              " 'encoder_2/bidirectional_rnn/fw/lstm_cell/bias/Initializer/zeros/Const',\n",
              " 'encoder_2/bidirectional_rnn/fw/lstm_cell/bias/Initializer/zeros',\n",
              " 'encoder_2/bidirectional_rnn/fw/lstm_cell/bias',\n",
              " 'encoder_2/bidirectional_rnn/fw/lstm_cell/bias/Assign',\n",
              " 'encoder_2/bidirectional_rnn/fw/lstm_cell/bias/read',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/lstm_cell/concat/axis',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/lstm_cell/concat',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/lstm_cell/MatMul/Enter',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/lstm_cell/MatMul',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/lstm_cell/BiasAdd/Enter',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/lstm_cell/BiasAdd',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/lstm_cell/Const',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/lstm_cell/split/split_dim',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/lstm_cell/split',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/lstm_cell/add/y',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/lstm_cell/add',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/lstm_cell/Sigmoid',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/lstm_cell/mul',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/lstm_cell/Sigmoid_1',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/lstm_cell/Tanh',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/lstm_cell/mul_1',\n",
              " 'encoder_2/bidirectional_rnn/fw/fw/while/lstm_cell/add_1',\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNbcpvEMyT_c"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDObDkepyr3p"
      },
      "source": [
        "###Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjcRCEusytlG"
      },
      "source": [
        "def text_to_seq(text):\n",
        "    '''Prepare the text for the model'''\n",
        "    \n",
        "    text = clean_data(text)\n",
        "    return [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in text.split()]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3y-VfNpJyu_R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7922f3a0-60ca-4b87-96ce-dd275348fad6"
      },
      "source": [
        "# Create your own review or use one from the dataset\n",
        "input_sentence = \"\"\"UNDANG-UNDANG DASAR NEGARA REPUBLIK INDONESIA TAHUN 1945\n",
        "PEMBUKAAN\n",
        "( P r e a m b u l e)\n",
        "\n",
        "\n",
        "Bahwa sesungguhnya Kemerdekaan itu ialah hak segala bangsa dan oleh sebab itu, maka penjajahan di atas dunia harus dihapuskan, karena tidak sesuai dengan perikemanusiaan dan perikeadilan.\n",
        "\n",
        "\n",
        "Dan perjuangan pergerakan kemerdekaan Indonesia telah sampailah kepada saat yang berbahagia dengan selamat sentausa mengantarkan rakyat Indonesia ke depan pintu gerbang kemerdekaan Negara Indonesia, yang merdeka, bersatu, berdaulat, adil dan makmur.\n",
        "\n",
        "\n",
        "Atas berkat rakhmat Allah Yang Maha Kuasa dan dengan didorongkan oleh keinginan luhur, supaya berkehidupan kebangsaan yang bebas, maka rakyat Indonesia menyatakan dengan ini kemerdekaannya.\n",
        "\n",
        "\n",
        "Kemudian daripada itu untuk membentuk suatu Pemerintah Negara Indonesia yang melindungi segenap bangsa Indonesia dan seluruh tumpah darah Indonesia dan untuk memajukan kesejahteraan umum, mencerdaskan kehidupan bangsa, dan ikut melaksanakan ketertiban dunia yang berdasarkan kemerdekaan, perdamaian abadi dan keadilan sosial, maka disusunlah Kemerdekaan Kebangsaan Indonesia itu dalam suatu Undang-Undang Dasar Negara Indonesia, yang terbentuk dalam suatu susunan Negara Republik Indonesia yang berkedaulatan rakyat dengan berdasar kepada Ketuhanan Yang Maha Esa, Kemanusiaan yang adil dan beradab, Persatuan Indonesia dan Kerakyatan yang dipimpin oleh hikmat kebijaksanaan dalam Permusyawaratan/Perwakilan, serta dengan mewujudkan suatu Keadilan sosial bagi seluruh rakyat Indonesia.\"\"\"\n",
        "text = text_to_seq(input_sentence)\n",
        "# random = np.random.randint(0,len(clean_texts))\n",
        "# input_sentence = clean_texts[random]\n",
        "# text = text_to_seq(clean_texts[random])\n",
        "\n",
        "checkpoint = \"/content/drive/MyDrive/Checkpoints/summarizer\" \n",
        "\n",
        "loaded_graph = tf.Graph()\n",
        "with tf.Session(graph=loaded_graph) as sess:\n",
        "    # Load saved model\n",
        "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
        "    loader.restore(sess, checkpoint)\n",
        "\n",
        "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
        "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
        "    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
        "    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
        "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
        "\n",
        "    summary_len = round(len(text)/2)\n",
        "    if(summary_len > 55):\n",
        "      summary_len = 55\n",
        "\n",
        "    \n",
        "    #Multiply by batch_size to match the model's input parameters\n",
        "    answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
        "                                      summary_length: [summary_len], \n",
        "                                      text_length: [len(text)]*batch_size,\n",
        "                                      keep_prob: 1.0})[0] \n",
        "\n",
        "# Remove the padding from the tweet\n",
        "pad = vocab_to_int[\"<PAD>\"] \n",
        "\n",
        "# print('Original Text:', reviews.Text[random])\n",
        "# print('Original summary:', reviews.Summary[random])#clean_summaries[random]\n",
        "\n",
        "print('\\nText')\n",
        "print('  Word Ids:    {}'.format([i for i in text]))\n",
        "print('  Input Words: {}'.format(\" \".join([int_to_vocab[i] for i in text])))\n",
        "\n",
        "print('\\nSummary')\n",
        "print('  Word Ids:       {}'.format([i for i in answer_logits if i != pad]))\n",
        "print('  Response Words: {}'.format(\" \".join([int_to_vocab[i] for i in answer_logits if i != pad])))"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/drive/MyDrive/Checkpoints/summarizer\n",
            "\n",
            "Text\n",
            "  Word Ids:    [1883, 1883, 2121, 160, 3057, 27, 200, 2157, 2734, 10752, 3732, 110, 859, 2439, 1480, 1305, 8591, 110, 14413, 4428, 1202, 786, 2668, 184, 31816, 56, 445, 17235, 917, 36519, 64662, 4679, 1970, 4428, 27, 45718, 5726, 2528, 64662, 627, 4101, 27, 46, 589, 3125, 4428, 160, 27, 4429, 1451, 26718, 5991, 23118, 56, 3644, 33059, 6500, 2127, 510, 64662, 1758, 32191, 64662, 946, 1375, 4101, 27, 1113, 23361, 367, 635, 4055, 91, 160, 27, 3248, 13183, 184, 27, 677, 7797, 1687, 27, 10675, 2433, 1111, 31458, 2553, 184, 102, 3003, 12160, 445, 737, 4428, 4973, 7197, 2123, 1037, 64662, 4428, 946, 27, 4055, 1883, 1883, 2121, 160, 27, 11868, 4055, 15616, 160, 3057, 27, 64662, 4101, 12047, 16694, 2127, 2128, 9485, 5991, 34444, 1458, 27, 32748, 794, 23226, 37393, 6275, 1871, 7098, 4055, 2123, 1037, 677, 4101, 27]\n",
            "  Input Words: undang undang dasar negara republik indonesia tahun 1945 pembukaan p r e a m b u l e sesungguhnya kemerdekaan ialah hak segala bangsa penjajahan atas dunia dihapuskan sesuai perikemanusiaan <UNK> perjuangan pergerakan kemerdekaan indonesia sampailah berbahagia selamat <UNK> mengantarkan rakyat indonesia depan pintu gerbang kemerdekaan negara indonesia merdeka bersatu berdaulat adil makmur atas berkat rakhmat allah maha kuasa <UNK> keinginan luhur <UNK> kebangsaan bebas rakyat indonesia menyatakan kemerdekaannya kemudian membentuk suatu pemerintah negara indonesia melindungi segenap bangsa indonesia seluruh tumpah darah indonesia memajukan kesejahteraan umum mencerdaskan kehidupan bangsa ikut melaksanakan ketertiban dunia berdasarkan kemerdekaan perdamaian abadi keadilan sosial <UNK> kemerdekaan kebangsaan indonesia suatu undang undang dasar negara indonesia terbentuk suatu susunan negara republik indonesia <UNK> rakyat berdasar ketuhanan maha esa kemanusiaan adil beradab persatuan indonesia kerakyatan dipimpin hikmat kebijaksanaan permusyawaratan perwakilan mewujudkan suatu keadilan sosial seluruh rakyat indonesia\n",
            "\n",
            "Summary\n",
            "  Word Ids:       [2151, 2121, 160, 27, 200, 1077, 160, 27, 200, 1466, 160, 2121, 160, 3057, 27, 200, 2157, 1745, 4428, 3, 786, 2668, 184, 4101, 27, 15813, 200, 1077, 160, 3057, 27, 15813, 200, 2157, 1745, 4428, 3, 786, 2668, 184, 4101, 27, 15813, 200, 15813, 3057, 27, 15813, 15813, 15813, 15813, 15813, 15813, 15813, 15813]\n",
            "  Response Words: uu dasar negara indonesia tahun anggaran negara indonesia tahun jumlah negara dasar negara republik indonesia tahun 1945 menurutnya kemerdekaan merupakan hak segala bangsa rakyat indonesia nkri tahun anggaran negara republik indonesia nkri tahun 1945 menurutnya kemerdekaan merupakan hak segala bangsa rakyat indonesia nkri tahun nkri republik indonesia nkri nkri nkri nkri nkri nkri nkri nkri\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGxfPCLFR7WS",
        "outputId": "6b6b2d21-f43f-4c81-a86c-b60fa37cc812"
      },
      "source": [
        "for i in range(3):\n",
        "  print(vocab_to_int[\"undang\"])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1883\n",
            "1883\n",
            "1883\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yolk378z1rgK",
        "outputId": "dea7ea93-5e28-42d5-a48c-2eb563b8f904"
      },
      "source": [
        ""
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxwZGfad0bdE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}